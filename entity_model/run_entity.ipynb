{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2820c0-b76d-4e71-8ae0-b99ab7e969c4",
   "metadata": {},
   "source": [
    "## Running and evaluating the pre-trained entity model\n",
    "\n",
    "In this notebook we will build on our the classes and functions we defined in the entity_setup notebook to run and evalute the entity model proposed in the research paper [A Frustratingly Easy Approach for Entity and Relation Extraction](https://arxiv.org/pdf/2010.12812.pdf).\n",
    "\n",
    "This is a reproduction based on the instructions left by the authors in their [GitHub repo](https://github.com/princeton-nlp/PURE)\n",
    "\n",
    "We will run the entity model on the SchiERC dataset using a pre-trained BERT based nodel.\n",
    "\n",
    "The output of this notebook, a JSON file where keys are document and sentence indices, and values are lists of predicted entities in the format [start, end, label], will be used as the input for the relation model in the notebook `run_relation`\n",
    "\n",
    "\n",
    "Note that we haven't trained our own model yet. We will do that in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85774f-b0ca-4322-af3b-2fec51f5439a",
   "metadata": {},
   "source": [
    "### Basic setup\n",
    "\n",
    "First we need to run our work in the notebook `entity_setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c487caa8-073a-471d-b056-16f54fd534b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run entity_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb572a-87ab-4544-9999-f39018c88e2f",
   "metadata": {},
   "source": [
    "Then we degine a variable `task_ner_labels`, it's a dictionarry mapping each dataset to its entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2f8486-6867-4fd8-a7ff-7250ec40f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ner_labels = {\n",
    "    'ace04': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n",
    "    'ace05': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n",
    "    'scierc': ['Method', 'OtherScientificTerm', 'Task', 'Generic', 'Material', 'Metric'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a4c3b-f5d2-4ffc-b33b-3e4ec5949870",
   "metadata": {},
   "source": [
    "Then we define some variables:\n",
    "- `data_dir`: The directory in which our input data is stored.\n",
    "- `output_dir`: The directory to which to write  the output of the mnodel.\n",
    "- `task`: The task that the model will be used to make predictions on. \n",
    "- max_span_length: The maximum length of spans to consider. \n",
    "- context_window: The size of the context window to consider around each sentence.\n",
    "- eval_batch_size: The batch size of the samples.\n",
    "- test_pred_filename: The name of the prediction output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b069ca-18d9-4075-87d3-1d0d13e0f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/scierc_data/processed_data/json'\n",
    "output_dir = os.getcwd() + '/scierc_models/ent-scib-ctx0/'\n",
    "task = 'scierc'\n",
    "max_span_length = 8\n",
    "context_window = 0\n",
    "eval_batch_size = 32\n",
    "test_pred_filename = 'ent_pred_test.json'\n",
    "dev_pred_filename = 'ent_pred_dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d1e65-35ed-4f3d-81fd-60b4b9a93dce",
   "metadata": {},
   "source": [
    "### Running and evaluationg the pre-trained model\n",
    "\n",
    "Now that the setup is out of the way. We can actually run the model and evaluate it with a pre-trained BERT-based model on the SciERC dataset.\n",
    "\n",
    "#### Data File Paths:\n",
    "Since the SciERC dataset is already split into a training, development, and test set. We don't need to perform any split. So let's just load set the paths to the data files dowanloaded with the dataset.\n",
    "\n",
    "The input data format of the entity model is JSONL. Each line of the input file contains one document in the following format.\n",
    "```json\n",
    "{\n",
    "  # document ID (please make sure doc_key can be used to identify a certain document)\n",
    "  \"doc_key\": \"CNN_ENG_20030306_083604.6\",\n",
    "\n",
    "  # sentences in the document, each sentence is a list of tokens\n",
    "  \"sentences\": [\n",
    "    [...],\n",
    "    [...],\n",
    "    [\"tens\", \"of\", \"thousands\", \"of\", \"college\", ...],\n",
    "    ...\n",
    "  ],\n",
    "\n",
    "  # entities (boundaries and entity type) in each sentence\n",
    "  \"ner\": [\n",
    "    [...],\n",
    "    [...],\n",
    "    [[26, 26, \"LOC\"], [14, 14, \"PER\"], ...], #the boundary positions are indexed in the document level\n",
    "    ...,\n",
    "  ],\n",
    "\n",
    "  # relations (two spans and relation type) in each sentence\n",
    "  \"relations\": [\n",
    "    [...],\n",
    "    [...],\n",
    "    [[14, 14, 10, 10, \"ORG-AFF\"], [14, 14, 12, 13, \"ORG-AFF\"], ...],\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe19c86-0a03-46ee-85e1-1a794f84919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = os.path.join(data_dir, 'train.json')\n",
    "dev_data = os.path.join(data_dir, 'dev.json')\n",
    "test_data = os.path.join(data_dir, 'test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540da85-feeb-4269-8a58-8aa9a30d80e5",
   "metadata": {},
   "source": [
    "#### Output Directory Check\n",
    "\n",
    "Then, just to be safe, we check if the specified output directory (`output_dir`) exists. If not, we create the directory. This ensures that the output directory is available for storing model checkpoints, predictions, or other outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f905f0-dcf6-4435-889c-86f2cc21b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34ad66-e99e-4a89-89cd-cb4d7b31336b",
   "metadata": {},
   "source": [
    "#### NER Label Mapping\n",
    "\n",
    "The `get_labelmap` function is used to get the mapping for the SchiREC task as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d478bec-612d-4431-9668-145a59a627cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_label2id, ner_id2label = get_labelmap(task_ner_labels[task])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6fbcb1-0b3c-4d84-9f43-166078354d61",
   "metadata": {},
   "source": [
    "#### Development Dataset Processing\n",
    "\n",
    "The development dataset (`dev_data`) is loaded into a `Dataset` object. Then, it is processed using the `convert_dataset_to_samples` function to obtain samples and NER labels. The samples are batchified using the `batchify` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59bf65e9-fad8-4663-950e-25cab705bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/04/2024 20:41:36 - INFO - root - # Overlap: 0\n",
      "01/04/2024 20:41:36 - INFO - root - Extracted 275 samples from 50 documents, with 811 NER labels, 23.713 avg input length, 68 max length\n",
      "01/04/2024 20:41:36 - INFO - root - Max Length: 68, max NER: 11\n"
     ]
    }
   ],
   "source": [
    "dev_data = Dataset(dev_data)\n",
    "dev_samples, dev_ner = convert_dataset_to_samples(dev_data, max_span_length, ner_label2id=ner_label2id, context_window=context_window)\n",
    "dev_batches = batchify(dev_samples, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96822b91-d8f4-4bc9-bd7b-12c339acca82",
   "metadata": {},
   "source": [
    "#### Model Initialization\n",
    "\n",
    "The BERT-based entity model (`EntityModel`) is initialized with specific parameters, including the BERT model name (`allenai/scibert_scivocab_uncased`), output directory for saving checkpoints (`bert_model_dir`), and the number of NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac52da30-c284-4a48-b6d4-cc2565e8812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/23/2023 13:33:05 - INFO - root - Loading BERT model from C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - Model name 'C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - Didn't find file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//added_tokens.json. We won't load it.\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - Didn't find file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//tokenizer.json. We won't load it.\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - loading file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//vocab.txt\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - loading file None\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - loading file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//special_tokens_map.json\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - loading file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//tokenizer_config.json\n",
      "11/23/2023 13:33:05 - INFO - transformers.tokenization_utils_base - loading file None\n",
      "11/23/2023 13:33:05 - INFO - transformers.configuration_utils - loading configuration file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//config.json\n",
      "11/23/2023 13:33:05 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertNerRe\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "11/23/2023 13:33:05 - INFO - transformers.modeling_utils - loading weights file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//pytorch_model.bin\n",
      "11/23/2023 13:33:08 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BertForEntity.\n",
      "\n",
      "11/23/2023 13:33:08 - INFO - transformers.modeling_utils - All the weights of BertForEntity were initialized from the model checkpoint at C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForEntity for predictions without further training.\n",
      "11/23/2023 13:33:08 - INFO - root - Moving to CUDA...\n",
      "11/23/2023 13:33:08 - INFO - root - # GPUs = 1\n"
     ]
    }
   ],
   "source": [
    "bert_model_dir = output_dir\n",
    "num_ner_labels = len(task_ner_labels[task]) + 1\n",
    "model = EntityModel(model='allenai/scibert_scivocab_uncased', bert_model_dir=bert_model_dir, use_albert=False, max_span_length=max_span_length, num_ner_labels=num_ner_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b65c7f-f07d-438a-980f-05f96e418821",
   "metadata": {},
   "source": [
    "#### Test Dataset Processing and Evaluation\n",
    "\n",
    "Finally the test dataset (`test_data`) is loaded, processed, and batchified similarly to the development dataset. The model is then evaluated on the test data using the `evaluate` function, and the NER predictions are saved to a file using the `output_ner_predictions` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f86cef2-d427-47d7-a3f1-1852cf1313d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/23/2023 13:33:13 - INFO - root - # Overlap: 0\n",
      "11/23/2023 13:33:13 - INFO - root - Extracted 551 samples from 100 documents, with 1685 NER labels, 24.321 avg input length, 97 max length\n",
      "11/23/2023 13:33:13 - INFO - root - Max Length: 97, max NER: 13\n",
      "11/23/2023 13:33:13 - INFO - root - Evaluating...\n",
      "11/23/2023 13:33:28 - INFO - root - Accuracy: 0.990194\n",
      "11/23/2023 13:33:28 - INFO - root - Cor: 1122, Pred TOT: 1680, Gold TOT: 1685\n",
      "11/23/2023 13:33:28 - INFO - root - P: 0.66786, R: 0.66588, F1: 0.66686\n",
      "11/23/2023 13:33:28 - INFO - root - Used time: 15.231171\n",
      "11/23/2023 13:33:41 - INFO - root - Total pred entities: 1680\n",
      "11/23/2023 13:33:41 - INFO - root - Output predictions to C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0/ent_pred_test.json..\n"
     ]
    }
   ],
   "source": [
    "test_data = Dataset(test_data)\n",
    "prediction_file = os.path.join(output_dir, test_pred_filename)\n",
    "\n",
    "test_samples, test_ner = convert_dataset_to_samples(test_data, max_span_length, ner_label2id=ner_label2id, context_window=context_window)\n",
    "test_batches = batchify(test_samples, eval_batch_size)\n",
    "evaluate(model, test_batches, test_ner)\n",
    "output_ner_predictions(model, test_batches, test_data, output_file=prediction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be12cc3-dfe4-4bf0-8151-59a0924e95ba",
   "metadata": {},
   "source": [
    "### Results \n",
    "**Accuracy**: 0.990194\n",
    "\n",
    "**Correct Predictions**: 1122\\\n",
    "**Total Predictions**: 1680\\\n",
    "**Total Gold Entities**: 1685\n",
    "\n",
    "**Precision**: 0.66786\\\n",
    "**Recall**: 0.66588\\\n",
    "**F1 Score**: 0.66686\n",
    "\n",
    "**Implications**:\n",
    "\n",
    "- The model is performing very well, with a high overall accuracy.\n",
    "- Precision, recall, and F1 score are reasonably balanced, indicating that the model is achieving a good trade-off between precision and recall.\n",
    "- The model is correctly identifying entities in the text, but there is still room for improvement, as evidenced by the not-perfect precision and recall values.\n",
    "- The results suggest that the model is generalizing well to new data, as indicated by the high accuracy.\n",
    "\n",
    "**Note** \n",
    "It's important to note that the interpretation of these metrics can depend on the specific requirements and priorities of the NER task at hand. For example, in some cases, precision may be more critical than recall, or vice versa, depending on the consequences of false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d76cb-3f0c-4822-bca5-d0df0d680434",
   "metadata": {},
   "source": [
    "## Training and evaluating the entity model from scratch\n",
    "Now we will train and evaluate an entity model from scratch in the notebook train_entity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
