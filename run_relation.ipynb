{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fde0f-a83b-49d3-b02b-44568a108606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c71fcf9-94fa-4453-b766-0052e7fc38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_marker_tokens(tokenizer, ner_labels):\n",
    "    new_tokens = ['<SUBJ_START>', '<SUBJ_END>', '<OBJ_START>', '<OBJ_END>']\n",
    "    for label in ner_labels:\n",
    "        new_tokens.append('<SUBJ_START=%s>'%label)\n",
    "        new_tokens.append('<SUBJ_END=%s>'%label)\n",
    "        new_tokens.append('<OBJ_START=%s>'%label)\n",
    "        new_tokens.append('<OBJ_END=%s>'%label)\n",
    "    for label in ner_labels:\n",
    "        new_tokens.append('<SUBJ=%s>'%label)\n",
    "        new_tokens.append('<OBJ=%s>'%label)\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "    logger.info('# vocab after adding markers: %d'%len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdae56d-073e-4ca4-ba7f-4f71ff447acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label2id, max_seq_length, tokenizer, special_tokens, unused_tokens=True):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of `InputBatch`s.\n",
    "    unused_tokens: whether use [unused1] [unused2] as special tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def get_special_token(w):\n",
    "        if w not in special_tokens:\n",
    "            if unused_tokens:\n",
    "                special_tokens[w] = \"[unused%d]\" % (len(special_tokens) + 1)\n",
    "            else:\n",
    "                special_tokens[w] = ('<' + w + '>').lower()\n",
    "        return special_tokens[w]\n",
    "\n",
    "    num_tokens = 0\n",
    "    max_tokens = 0\n",
    "    num_fit_examples = 0\n",
    "    num_shown_examples = 0\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens = [CLS]\n",
    "        SUBJECT_START = get_special_token(\"SUBJ_START\")\n",
    "        SUBJECT_END = get_special_token(\"SUBJ_END\")\n",
    "        OBJECT_START = get_special_token(\"OBJ_START\")\n",
    "        OBJECT_END = get_special_token(\"OBJ_END\")\n",
    "        SUBJECT_NER = get_special_token(\"SUBJ=%s\" % example['subj_type'])\n",
    "        OBJECT_NER = get_special_token(\"OBJ=%s\" % example['obj_type'])\n",
    "\n",
    "        SUBJECT_START_NER = get_special_token(\"SUBJ_START=%s\"%example['subj_type'])\n",
    "        SUBJECT_END_NER = get_special_token(\"SUBJ_END=%s\"%example['subj_type'])\n",
    "        OBJECT_START_NER = get_special_token(\"OBJ_START=%s\"%example['obj_type'])\n",
    "        OBJECT_END_NER = get_special_token(\"OBJ_END=%s\"%example['obj_type'])\n",
    "\n",
    "        for i, token in enumerate(example['token']):\n",
    "            if i == example['subj_start']:\n",
    "                sub_idx = len(tokens)\n",
    "                tokens.append(SUBJECT_START_NER)\n",
    "            if i == example['obj_start']:\n",
    "                obj_idx = len(tokens)\n",
    "                tokens.append(OBJECT_START_NER)\n",
    "            for sub_token in tokenizer.tokenize(token):\n",
    "                tokens.append(sub_token)\n",
    "            if i == example['subj_end']:\n",
    "                tokens.append(SUBJECT_END_NER)\n",
    "            if i == example['obj_end']:\n",
    "                tokens.append(OBJECT_END_NER)\n",
    "        tokens.append(SEP)\n",
    "\n",
    "        num_tokens += len(tokens)\n",
    "        max_tokens = max(max_tokens, len(tokens))\n",
    "\n",
    "        if len(tokens) > max_seq_length:\n",
    "            tokens = tokens[:max_seq_length]\n",
    "            if sub_idx >= max_seq_length:\n",
    "                sub_idx = 0\n",
    "            if obj_idx >= max_seq_length:\n",
    "                obj_idx = 0\n",
    "        else:\n",
    "            num_fit_examples += 1\n",
    "\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        label_id = label2id[example['relation']]\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if num_shown_examples < 20:\n",
    "            if (ex_index < 5) or (label_id > 0):\n",
    "                num_shown_examples += 1\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"guid: %s\" % (example['id']))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(\n",
    "                        [str(x) for x in tokens]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                logger.info(\"label: %s (id = %d)\" % (example['relation'], label_id))\n",
    "                logger.info(\"sub_idx, obj_idx: %d, %d\" % (sub_idx, obj_idx))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id,\n",
    "                              sub_idx=sub_idx,\n",
    "                              obj_idx=obj_idx))\n",
    "    logger.info(\"Average #tokens: %.2f\" % (num_tokens * 1.0 / len(examples)))\n",
    "    logger.info(\"Max #tokens: %d\"%max_tokens)\n",
    "    logger.info(\"%d (%.2f %%) examples can fit max_seq_length = %d\" % (num_fit_examples,\n",
    "                num_fit_examples * 100.0 / len(examples), max_seq_length))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f309e0c-02b8-47ec-a482-62cb017c9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e72b43a-4325-4eb3-90ce-091701eb3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(preds, labels, e2e_ngold):\n",
    "    n_gold = n_pred = n_correct = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        if pred != 0:\n",
    "            n_pred += 1\n",
    "        if label != 0:\n",
    "            n_gold += 1\n",
    "        if (pred != 0) and (label != 0) and (pred == label):\n",
    "            n_correct += 1\n",
    "    if n_correct == 0:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    else:\n",
    "        prec = n_correct * 1.0 / n_pred\n",
    "        recall = n_correct * 1.0 / n_gold\n",
    "        if prec + recall > 0:\n",
    "            f1 = 2.0 * prec * recall / (prec + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "        if e2e_ngold is not None:\n",
    "            e2e_recall = n_correct * 1.0 / e2e_ngold\n",
    "            e2e_f1 = 2.0 * prec * e2e_recall / (prec + e2e_recall)\n",
    "        else:\n",
    "            e2e_recall = e2e_f1 = 0.0\n",
    "        return {'precision': prec, 'recall': e2e_recall, 'f1': e2e_f1, 'task_recall': recall, 'task_f1': f1, \n",
    "        'n_correct': n_correct, 'n_pred': n_pred, 'n_gold': e2e_ngold, 'task_ngold': n_gold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1953e6e9-48cb-49e1-8494-77db5028f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, eval_dataloader, eval_label_ids, num_labels, e2e_ngold=None, verbose=True):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "    for input_ids, input_mask, segment_ids, label_ids, sub_idx, obj_idx in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        sub_idx = sub_idx.to(device)\n",
    "        obj_idx = obj_idx.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None, sub_idx=sub_idx, obj_idx=obj_idx)\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    logits = preds[0]\n",
    "    preds = np.argmax(preds[0], axis=1)\n",
    "    result = compute_f1(preds, eval_label_ids.numpy(), e2e_ngold=e2e_ngold)\n",
    "    result['accuracy'] = simple_accuracy(preds, eval_label_ids.numpy())\n",
    "    result['eval_loss'] = eval_loss\n",
    "    if verbose:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    return preds, result, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39af037c-b43b-4006-aa64-29d4e0059140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_json(eval_data, eval_examples, preds, id2label, output_file):\n",
    "    rels = dict()\n",
    "    for ex, pred in zip(eval_examples, preds):\n",
    "        doc_sent, sub, obj = decode_sample_id(ex['id'])\n",
    "        if doc_sent not in rels:\n",
    "            rels[doc_sent] = []\n",
    "        if pred != 0:\n",
    "            rels[doc_sent].append([sub[0], sub[1], obj[0], obj[1], id2label[pred]])\n",
    "\n",
    "    js = eval_data.js\n",
    "    for doc in js:\n",
    "        doc['predicted_relations'] = []\n",
    "        for sid in range(len(doc['sentences'])):\n",
    "            k = '%s@%d'%(doc['doc_key'], sid)\n",
    "            doc['predicted_relations'].append(rels.get(k, []))\n",
    "    \n",
    "    logger.info('Output predictions to %s..'%(output_file))\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(json.dumps(doc) for doc in js))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0c604f-380e-4f41-a5b0-62b59ba851b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setseed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba02b22-fe74-438e-86f1-5f9fa3c02f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(output_dir, model, tokenizer):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    logger.info('Saving model to %s'%output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    model_to_save.config.to_json_file(output_config_file)\n",
    "    tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68ae9e0-5769-48e5-bc42-0cf01efca832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\odaim\\anaconda3\\envs\\PUREReprodcution\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers import AlbertModel, AlbertPreTrainedModel\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a60c06-d042-4fa9-a2a0-8daba9d62f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForRelation(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_rel_labels):\n",
    "        super(BertForRelation, self).__init__(config)\n",
    "        self.num_labels = num_rel_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.layer_norm = BertLayerNorm(config.hidden_size * 2)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, sub_idx=None, obj_idx=None, input_position=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=False, output_attentions=False, position_ids=input_position)\n",
    "        sequence_output = outputs[0]\n",
    "        sub_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, sub_idx)])\n",
    "        obj_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, obj_idx)])\n",
    "        rep = torch.cat((sub_output, obj_output), dim=1)\n",
    "        rep = self.layer_norm(rep)\n",
    "        rep = self.dropout(rep)\n",
    "        logits = self.classifier(rep)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33aafec3-d7f0-4a7c-9472-1a574176cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertForRelation(AlbertPreTrainedModel):\n",
    "    def __init__(self, config, num_rel_labels):\n",
    "        super(AlbertForRelation, self).__init__(config)\n",
    "        self.num_labels = num_rel_labels\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.layer_norm = BertLayerNorm(config.hidden_size * 2)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, sub_idx=None, obj_idx=None):\n",
    "        outputs = self.albert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=False, output_attentions=False)\n",
    "        sequence_output = outputs[0]\n",
    "        sub_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, sub_idx)])\n",
    "        obj_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, obj_idx)])\n",
    "        rep = torch.cat((sub_output, obj_output), dim=1)\n",
    "        rep = self.layer_norm(rep)\n",
    "        rep = self.dropout(rep)\n",
    "        logits = self.classifier(rep)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c5f983-c388-42e4-97b7-0504ef40a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation_data(entity_data, use_gold=False, context_window=0):\n",
    "    \"\"\"\n",
    "    Prepare data for the relation model\n",
    "    If training: set use_gold = True\n",
    "    \"\"\"\n",
    "    logger.info('Generate relation data from %s'%(entity_data))\n",
    "    data = Dataset(entity_data)\n",
    "\n",
    "    nner, nrel = 0, 0\n",
    "    max_sentsample = 0\n",
    "    samples = []\n",
    "    for doc in data:\n",
    "        for i, sent in enumerate(doc):\n",
    "            sent_samples = []\n",
    "\n",
    "            nner += len(sent.ner)\n",
    "            nrel += len(sent.relations)\n",
    "            if use_gold:\n",
    "                sent_ner = sent.ner\n",
    "            else:\n",
    "                sent_ner = sent.predicted_ner\n",
    "            \n",
    "            gold_ner = {}\n",
    "            for ner in sent.ner:\n",
    "                gold_ner[ner.span] = ner.label\n",
    "            \n",
    "            gold_rel = {}\n",
    "            for rel in sent.relations:\n",
    "                gold_rel[rel.pair] = rel.label\n",
    "            \n",
    "            sent_start = 0\n",
    "            sent_end = len(sent.text)\n",
    "            tokens = sent.text\n",
    "\n",
    "            if context_window > 0:\n",
    "                add_left = (context_window-len(sent.text)) // 2\n",
    "                add_right = (context_window-len(sent.text)) - add_left\n",
    "\n",
    "                j = i - 1\n",
    "                while j >= 0 and add_left > 0:\n",
    "                    context_to_add = doc[j].text[-add_left:]\n",
    "                    tokens = context_to_add + tokens\n",
    "                    add_left -= len(context_to_add)\n",
    "                    sent_start += len(context_to_add)\n",
    "                    sent_end += len(context_to_add)\n",
    "                    j -= 1\n",
    "\n",
    "                j = i + 1\n",
    "                while j < len(doc) and add_right > 0:\n",
    "                    context_to_add = doc[j].text[:add_right]\n",
    "                    tokens = tokens + context_to_add\n",
    "                    add_right -= len(context_to_add)\n",
    "                    j += 1\n",
    "            \n",
    "            for x in range(len(sent_ner)):\n",
    "                for y in range(len(sent_ner)):\n",
    "                    if x == y:\n",
    "                        continue\n",
    "                    sub = sent_ner[x]\n",
    "                    obj = sent_ner[y]\n",
    "                    label = gold_rel.get((sub.span, obj.span), 'no_relation')\n",
    "                    sample = {}\n",
    "                    sample['docid'] = doc._doc_key\n",
    "                    sample['id'] = '%s@%d::(%d,%d)-(%d,%d)'%(doc._doc_key, sent.sentence_ix, sub.span.start_doc, sub.span.end_doc, obj.span.start_doc, obj.span.end_doc)\n",
    "                    sample['relation'] = label\n",
    "                    sample['subj_start'] = sub.span.start_sent + sent_start\n",
    "                    sample['subj_end'] = sub.span.end_sent + sent_start\n",
    "                    sample['subj_type'] = sub.label\n",
    "                    sample['obj_start'] = obj.span.start_sent + sent_start\n",
    "                    sample['obj_end'] = obj.span.end_sent + sent_start\n",
    "                    sample['obj_type'] = obj.label\n",
    "                    sample['token'] = tokens\n",
    "                    sample['sent_start'] = sent_start\n",
    "                    sample['sent_end'] = sent_end\n",
    "\n",
    "                    sent_samples.append(sample)\n",
    "\n",
    "            max_sentsample = max(max_sentsample, len(sent_samples))\n",
    "            samples += sent_samples\n",
    "    \n",
    "    tot = len(samples)\n",
    "    logger.info('#samples: %d, max #sent.samples: %d'%(tot, max_sentsample))\n",
    "\n",
    "    return data, samples, nrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73432310-8161-4745-b55a-f93e909e4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, json_file, pred_file=None, doc_range=None):\n",
    "        self.js = self._read(json_file, pred_file)\n",
    "        if doc_range is not None:\n",
    "            self.js = self.js[doc_range[0]:doc_range[1]]\n",
    "        self.documents = [Document(js) for js in self.js]\n",
    "\n",
    "    def update_from_js(self, js):\n",
    "        self.js = js\n",
    "        self.documents = [Document(js) for js in self.js]\n",
    "\n",
    "    def _read(self, json_file, pred_file=None):\n",
    "        gold_docs = [json.loads(line) for line in open(json_file)]\n",
    "        if pred_file is None:\n",
    "            return gold_docs\n",
    "\n",
    "        pred_docs = [json.loads(line) for line in open(pred_file)]\n",
    "        merged_docs = []\n",
    "        for gold, pred in zip(gold_docs, pred_docs):\n",
    "            assert gold[\"doc_key\"] == pred[\"doc_key\"]\n",
    "            assert gold[\"sentences\"] == pred[\"sentences\"]\n",
    "            merged = copy.deepcopy(gold)\n",
    "            for k, v in pred.items():\n",
    "                if \"predicted\" in k:\n",
    "                    merged[k] = v\n",
    "            merged_docs.append(merged)\n",
    "\n",
    "        return merged_docs\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.documents[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3030363c-9fab-41ec-a724-e5a7b404d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, entry, sentence_start, sentence_ix):\n",
    "        self.sentence_start = sentence_start\n",
    "        self.text = entry[\"sentences\"]\n",
    "        self.sentence_ix = sentence_ix\n",
    "        # Gold\n",
    "        if \"ner_flavor\" in entry:\n",
    "            self.ner = [NER(this_ner, self.text, sentence_start, flavor=this_flavor)\n",
    "                        for this_ner, this_flavor in zip(entry[\"ner\"], entry[\"ner_flavor\"])]\n",
    "        elif \"ner\" in entry:\n",
    "            self.ner = [NER(this_ner, self.text, sentence_start)\n",
    "                        for this_ner in entry[\"ner\"]]\n",
    "        if \"relations\" in entry:\n",
    "            self.relations = [Relation(this_relation, self.text, sentence_start) for\n",
    "                              this_relation in entry[\"relations\"]]\n",
    "        if \"events\" in entry:\n",
    "            self.events = Events(entry[\"events\"], self.text, sentence_start)\n",
    "\n",
    "        # Predicted\n",
    "        if \"predicted_ner\" in entry:\n",
    "            self.predicted_ner = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
    "                                  this_ner in entry[\"predicted_ner\"]]\n",
    "        if \"predicted_relations\" in entry:\n",
    "            self.predicted_relations = [Relation(this_relation, self.text, sentence_start) for\n",
    "                                        this_relation in entry[\"predicted_relations\"]]\n",
    "        if \"predicted_events\" in entry:\n",
    "            self.predicted_events = Events(entry[\"predicted_events\"], self.text, sentence_start)\n",
    "\n",
    "        # Top spans\n",
    "        if \"top_spans\" in entry:\n",
    "            self.top_spans = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
    "                                this_ner in entry[\"top_spans\"]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        the_text = \" \".join(self.text)\n",
    "        the_lengths = np.array([len(x) for x in self.text])\n",
    "        tok_ixs = \"\"\n",
    "        for i, offset in enumerate(the_lengths):\n",
    "            true_offset = offset if i < 10 else offset - 1\n",
    "            tok_ixs += str(i)\n",
    "            tok_ixs += \" \" * true_offset\n",
    "\n",
    "        return the_text + \"\\n\" + tok_ixs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def get_flavor(self, argument):\n",
    "        the_ner = [x for x in self.ner if x.span == argument.span]\n",
    "        if len(the_ner) > 1:\n",
    "            print(\"Weird\")\n",
    "        if the_ner:\n",
    "            the_flavor = the_ner[0].flavor\n",
    "        else:\n",
    "            the_flavor = None\n",
    "        return the_flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd3ebedd-f439-4d22-a812-560ca07efe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, js):\n",
    "        self._doc_key = js[\"doc_key\"]\n",
    "        entries = fields_to_batches(js, [\"doc_key\", \"clusters\", \"predicted_clusters\", \"section_starts\"])\n",
    "        sentence_lengths = [len(entry[\"sentences\"]) for entry in entries]\n",
    "        sentence_starts = np.cumsum(sentence_lengths)\n",
    "        sentence_starts = np.roll(sentence_starts, 1)\n",
    "        sentence_starts[0] = 0\n",
    "        self.sentence_starts = sentence_starts\n",
    "        self.sentences = [Sentence(entry, sentence_start, sentence_ix)\n",
    "                          for sentence_ix, (entry, sentence_start)\n",
    "                          in enumerate(zip(entries, sentence_starts))]\n",
    "        if \"clusters\" in js:\n",
    "            self.clusters = [Cluster(entry, i, self)\n",
    "                             for i, entry in enumerate(js[\"clusters\"])]\n",
    "        if \"predicted_clusters\" in js:\n",
    "            self.predicted_clusters = [Cluster(entry, i, self)\n",
    "                                       for i, entry in enumerate(js[\"predicted_clusters\"])]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([str(i) + \": \" + \" \".join(sent.text) for i, sent in enumerate(self.sentences)])\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def print_plaintext(self):\n",
    "        for sent in self:\n",
    "            print(\" \".join(sent.text))\n",
    "\n",
    "\n",
    "    def find_cluster(self, entity, predicted=True):\n",
    "        \"\"\"\n",
    "        Search through erence clusters and return the one containing the query entity, if it's\n",
    "        part of a cluster. If we don't find a match, return None.\n",
    "        \"\"\"\n",
    "        clusters = self.predicted_clusters if predicted else self.clusters\n",
    "        for clust in clusters:\n",
    "            for entry in clust:\n",
    "                if entry.span == entity.span:\n",
    "                    return clust\n",
    "\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return sum([len(sent) for sent in self.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d638650-dbb5-4289-99b5-52ff47bff05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER:\n",
    "    def __init__(self, ner, text, sentence_start, flavor=None):\n",
    "        self.span = Span(ner[0], ner[1], text, sentence_start)\n",
    "        self.label = ner[2]\n",
    "        self.flavor = flavor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.span.__repr__() + \": \" + self.label\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.span == other.span and\n",
    "                self.label == other.label and\n",
    "                self.flavor == other.flavor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbea2e72-2d85-44e1-9de5-fa9f7d4d5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, start, end, text, sentence_start):\n",
    "        self.start_doc = start\n",
    "        self.end_doc = end\n",
    "        self.span_doc = (self.start_doc, self.end_doc)\n",
    "        self.start_sent = start - sentence_start\n",
    "        self.end_sent = end - sentence_start\n",
    "        self.span_sent = (self.start_sent, self.end_sent)\n",
    "        self.text = text[self.start_sent:self.end_sent + 1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str((self.start_sent, self.end_sent, self.text))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.span_doc == other.span_doc and\n",
    "                self.span_sent == other.span_sent and\n",
    "                self.text == other.text)\n",
    "\n",
    "    def __hash__(self):\n",
    "        tup = self.span_doc + self.span_sent + (\" \".join(self.text),)\n",
    "        return hash(tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c87a147-2a87-4b2b-a7de-d0f14b5b84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relation:\n",
    "    def __init__(self, relation, text, sentence_start):\n",
    "        start1, end1 = relation[0], relation[1]\n",
    "        start2, end2 = relation[2], relation[3]\n",
    "        label = relation[4]\n",
    "        span1 = Span(start1, end1, text, sentence_start)\n",
    "        span2 = Span(start2, end2, text, sentence_start)\n",
    "        self.pair = (span1, span2)\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.pair[0].__repr__() + \", \" + self.pair[1].__repr__() + \": \" + self.label\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.pair == other.pair) and (self.label == other.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf27b5db-6ece-4a9e-9e13-24e60c57ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, cluster, cluster_id, document):\n",
    "        members = []\n",
    "        for entry in cluster:\n",
    "            sentence_ix = get_sentence_of_span(entry, document.sentence_starts, document.n_tokens)\n",
    "            sentence = document[sentence_ix]\n",
    "            span = Span(entry[0], entry[1], sentence.text, sentence.sentence_start)\n",
    "            ners = [x for x in sentence.ner if x.span == span]\n",
    "            assert len(ners) <= 1\n",
    "            ner = ners[0] if len(ners) == 1 else None\n",
    "            to_append = ClusterMember(span, ner, sentence, cluster_id)\n",
    "            members.append(to_append)\n",
    "\n",
    "        self.members = members\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.cluster_id}: \" + self.members.__repr__()\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.members[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97932254-c9ae-4c58-82f4-200ea937605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterMember:\n",
    "    def __init__(self, span, ner, sentence, cluster_id):\n",
    "        self.span = span\n",
    "        self.ner = ner\n",
    "        self.sentence = sentence\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.sentence.sentence_ix}> \" + self.span.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ecb42b4-7cfb-4739-b491-cd2f16529d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, sub_idx, obj_idx):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.sub_idx = sub_idx\n",
    "        self.obj_idx = obj_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3a955b1-2bcb-45d0-96ee-9389635d7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fields_to_batches(d, keys_to_ignore=[]):\n",
    "    keys = [key for key in d.keys() if key not in keys_to_ignore]\n",
    "    lengths = [len(d[k]) for k in keys]\n",
    "    assert len(set(lengths)) == 1\n",
    "    length = lengths[0]\n",
    "    res = [{k: d[k][i] for k in keys} for i in range(length)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98235e9b-c1d1-4dca-92b3-e146112e69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sample_id(sample_id):\n",
    "    doc_sent = sample_id.split('::')[0]\n",
    "    pair = sample_id.split('::')[1]\n",
    "    pair = pair.split('-')\n",
    "    sub = (int(pair[0][1:-1].split(',')[0]), int(pair[0][1:-1].split(',')[1]))\n",
    "    obj = (int(pair[1][1:-1].split(',')[0]), int(pair[1][1:-1].split(',')[1]))\n",
    "\n",
    "    return doc_sent, sub, obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f0452b4-e30b-4b14-986c-1ba8cc0f4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_of_span(span, sentence_starts, doc_tokens):\n",
    "    \"\"\"\n",
    "    Return the index of the sentence that the span is part of.\n",
    "    \"\"\"\n",
    "    # Inclusive sentence ends\n",
    "    sentence_ends = [x - 1 for x in sentence_starts[1:]] + [doc_tokens - 1]\n",
    "    in_between = [span[0] >= start and span[1] <= end\n",
    "                  for start, end in zip(sentence_starts, sentence_ends)]\n",
    "    assert sum(in_between) == 1\n",
    "    the_sentence = in_between.index(True)\n",
    "    return the_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d2ae96b-6dc1-44ba-b4c3-8a0ae9180eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b14b727-39e4-4328-a96f-6a10b50e65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "add_new_tokens = False\n",
    "no_cuda = False\n",
    "do_train = False\n",
    "do_eval = True\n",
    "eval_test = True\n",
    "do_lower_case = True\n",
    "entity_output_dir = os.getcwd() + '/scierc_models/ent-scib-ctx0/'\n",
    "entity_predictions_dev = 'ent_pred_dev.json'\n",
    "eval_with_gold = True\n",
    "context_window = 0\n",
    "max_seq_length = 128\n",
    "entity_predictions_test = 'ent_pred_test.json'\n",
    "seed = 0\n",
    "output_dir = os.getcwd() + '/scierc_models/rel_approx-scib-ctx0/'\n",
    "negative_label = 'no_relation'\n",
    "task = 'scierc'\n",
    "train_mode = 'random_sorted'\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "num_train_epochs = 3.0\n",
    "train_file = None\n",
    "eval_per_epoch = 10\n",
    "learning_rate = None\n",
    "prediction_file = 'predictions.json'\n",
    "BertLayerNorm = torch.nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e80d3b70-f753-45a1-b06d-ba7404ddc275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/13/2023 21:39:42 - INFO - root - Generate relation data from C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0/ent_pred_test.json\n",
      "11/13/2023 21:39:42 - INFO - root - #samples: 5062, max #sent.samples: 156\n",
      "11/13/2023 21:39:44 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/config.json from cache at C:\\Users\\odaim/.cache\\torch\\transformers\\199e28e62d2210c23d63625bd9eecc20cf72a156b29e2a540d4933af4f50bda1.4b6b9f5d813f7395e7ea533039e02deb1723d8fd9d8ba655391a01a69ad6223d\n",
      "11/13/2023 21:39:44 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "11/13/2023 21:39:44 - INFO - transformers.tokenization_utils_base - Model name 'allenai/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'allenai/scibert_scivocab_uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "11/13/2023 21:39:48 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/vocab.txt from cache at C:\\Users\\odaim/.cache\\torch\\transformers\\e3debd8fbdf40874753724814ee0520f612b577b26c8755bca485103b47cd3bc.60287becc5ab96d85a4bf377eb90feaf3b9c80d3b23e84311dccd3588f56d4fb\n",
      "11/13/2023 21:39:48 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/added_tokens.json from cache at None\n",
      "11/13/2023 21:39:48 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/special_tokens_map.json from cache at None\n",
      "11/13/2023 21:39:48 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/tokenizer_config.json from cache at None\n",
      "11/13/2023 21:39:48 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/tokenizer.json from cache at None\n",
      "11/13/2023 21:39:48 - INFO - root - {'SUBJ_START': '[unused1]', 'SUBJ_END': '[unused2]', 'OBJ_START': '[unused3]', 'OBJ_END': '[unused4]', 'SUBJ=Generic': '[unused5]', 'OBJ=OtherScientificTerm': '[unused6]', 'SUBJ_START=Generic': '[unused7]', 'SUBJ_END=Generic': '[unused8]', 'OBJ_START=OtherScientificTerm': '[unused9]', 'OBJ_END=OtherScientificTerm': '[unused10]', 'OBJ=Material': '[unused11]', 'OBJ_START=Material': '[unused12]', 'OBJ_END=Material': '[unused13]', 'SUBJ=OtherScientificTerm': '[unused14]', 'OBJ=Generic': '[unused15]', 'SUBJ_START=OtherScientificTerm': '[unused16]', 'SUBJ_END=OtherScientificTerm': '[unused17]', 'OBJ_START=Generic': '[unused18]', 'OBJ_END=Generic': '[unused19]', 'SUBJ=Material': '[unused20]', 'SUBJ_START=Material': '[unused21]', 'SUBJ_END=Material': '[unused22]', 'OBJ=Task': '[unused23]', 'OBJ_START=Task': '[unused24]', 'OBJ_END=Task': '[unused25]', 'SUBJ=Task': '[unused26]', 'SUBJ_START=Task': '[unused27]', 'SUBJ_END=Task': '[unused28]', 'OBJ=Method': '[unused29]', 'OBJ_START=Method': '[unused30]', 'OBJ_END=Method': '[unused31]', 'SUBJ=Method': '[unused32]', 'SUBJ_START=Method': '[unused33]', 'SUBJ_END=Method': '[unused34]', 'OBJ=Metric': '[unused35]', 'OBJ_START=Metric': '[unused36]', 'OBJ_END=Metric': '[unused37]', 'SUBJ=Metric': '[unused38]', 'SUBJ_START=Metric': '[unused39]', 'SUBJ_END=Metric': '[unused40]'}\n",
      "11/13/2023 21:39:48 - INFO - root - Writing example 0 of 5062\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(0,3)-(2,3)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused27] recognition of [unused9] proper nouns [unused28] [unused10] in japanese text has been studied as a part of the more general problem of morphological analysis in japanese text processing - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 28 3512 131 10 1193 24748 29 11 121 9155 3267 434 528 2580 188 106 1188 131 111 475 1196 1167 131 6893 669 121 9155 3267 2307 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: no_relation (id = 0)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 1, 4\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(0,3)-(5,6)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused27] recognition of proper nouns [unused28] in [unused12] japanese text [unused13] has been studied as a part of the more general problem of morphological analysis in japanese text processing - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 28 3512 131 1193 24748 29 121 13 9155 3267 14 434 528 2580 188 106 1188 131 111 475 1196 1167 131 6893 669 121 9155 3267 2307 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: no_relation (id = 0)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 1, 8\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(0,3)-(19,20)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused27] recognition of proper nouns [unused28] in japanese text has been studied as a part of the more general problem of [unused24] morphological analysis [unused25] in japanese text processing - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 28 3512 131 1193 24748 29 121 9155 3267 434 528 2580 188 106 1188 131 111 475 1196 1167 131 25 6893 669 26 121 9155 3267 2307 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: PART-OF (id = 1)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 1, 22\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(0,3)-(22,24)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused27] recognition of proper nouns [unused28] in japanese text has been studied as a part of the more general problem of morphological analysis in [unused24] japanese text processing [unused25] - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 28 3512 131 1193 24748 29 121 9155 3267 434 528 2580 188 106 1188 131 111 475 1196 1167 131 6893 669 121 25 9155 3267 2307 26 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: no_relation (id = 0)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 1, 25\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(2,3)-(0,3)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused24] recognition of [unused16] proper nouns [unused17] [unused25] in japanese text has been studied as a part of the more general problem of morphological analysis in japanese text processing - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 25 3512 131 17 1193 24748 18 26 121 9155 3267 434 528 2580 188 106 1188 131 111 475 1196 1167 131 6893 669 121 9155 3267 2307 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: no_relation (id = 0)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 4, 1\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(2,3)-(5,6)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] recognition of [unused16] proper nouns [unused17] in [unused12] japanese text [unused13] has been studied as a part of the more general problem of morphological analysis in japanese text processing - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 3512 131 17 1193 24748 18 121 13 9155 3267 14 434 528 2580 188 106 1188 131 111 475 1196 1167 131 6893 669 121 9155 3267 2307 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: PART-OF (id = 1)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 3, 8\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@0::(19,20)-(22,24)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] recognition of proper nouns in japanese text has been studied as a part of the more general problem of [unused27] morphological analysis [unused28] in [unused24] japanese text processing [unused25] - lr ##b - - ls ##b - 1 - rs ##b - - ls ##b - 2 - rs ##b - - rr ##b - . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 3512 131 1193 24748 121 9155 3267 434 528 2580 188 106 1188 131 111 475 1196 1167 131 28 6893 669 29 121 25 9155 3267 2307 26 579 8295 30125 579 579 6208 30125 579 158 579 3102 30125 579 579 6208 30125 579 170 579 3102 30125 579 579 5058 30125 579 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 20, 25\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@1::(43,45)-(34,34)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused18] it [unused19] has also been studied in the framework of [unused27] japanese information extraction [unused28] - lr ##b - - ls ##b - 3 - rs ##b - - rr ##b - in recent years . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 19 256 20 434 469 528 2580 121 111 2641 131 28 9155 776 4220 29 579 8295 30125 579 579 6208 30125 579 239 579 3102 30125 579 579 5058 30125 579 121 2151 1320 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 12, 1\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@2::(56,56)-(59,64)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our [unused7] approach [unused8] to the [unused24] multi - lingu ##al evaluation task - lr ##b - met - rr ##b - [unused25] for japanese text is to consider the given task as a morphological analysis problem in japanese . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 8 1139 9 147 111 25 869 579 8589 120 2166 2188 579 8295 30125 579 374 579 5058 30125 579 26 168 9155 3267 165 147 1129 111 906 2188 188 106 6893 669 1167 121 9155 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 2, 7\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@2::(59,64)-(66,67)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our approach to the [unused27] multi - lingu ##al evaluation task - lr ##b - met - rr ##b - [unused28] for [unused12] japanese text [unused13] is to consider the given task as a morphological analysis problem in japanese . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 1139 147 111 28 869 579 8589 120 2166 2188 579 8295 30125 579 374 579 5058 30125 579 29 168 13 9155 3267 14 165 147 1129 111 906 2188 188 106 6893 669 1167 121 9155 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 5, 23\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@2::(76,78)-(73,73)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our approach to the multi - lingu ##al evaluation task - lr ##b - met - rr ##b - for japanese text is to consider the given [unused18] task [unused19] as a [unused27] morphological analysis problem [unused28] in japanese . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 1139 147 111 869 579 8589 120 2166 2188 579 8295 30125 579 374 579 5058 30125 579 168 9155 3267 165 147 1129 111 906 19 2188 20 188 106 28 6893 669 1167 29 121 9155 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 33, 28\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@2::(80,80)-(76,78)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our approach to the multi - lingu ##al evaluation task - lr ##b - met - rr ##b - for japanese text is to consider the given task as a [unused24] morphological analysis problem [unused25] in [unused21] japanese [unused22] . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 1139 147 111 869 579 8589 120 2166 2188 579 8295 30125 579 374 579 5058 30125 579 168 9155 3267 165 147 1129 111 906 2188 188 106 25 6893 669 1167 26 121 22 9155 23 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 37, 31\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@3::(83,84)-(93,103)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our [unused33] morphological analyzer [unused34] has done all the necessary work for the [unused24] recognition and classification of proper names , numerical and temporal expressions [unused25] , i . e . named entity - lr ##b - ne - rr ##b - items in the japanese text . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 34 6893 12951 35 434 2992 355 111 2538 697 168 111 25 3512 137 2998 131 1193 8541 422 4058 137 3930 6370 26 422 259 205 139 205 8832 8494 579 8295 30125 579 287 579 5058 30125 579 3945 121 111 9155 3267 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 2, 14\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@3::(106,111)-(97,103)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our morphological analyzer has done all the necessary work for the recognition and classification of [unused9] proper names , numerical and temporal expressions [unused10] , i . e . [unused16] named entity - lr ##b - ne - rr ##b - items [unused17] in the japanese text . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 6893 12951 434 2992 355 111 2538 697 168 111 3512 137 2998 131 10 1193 8541 422 4058 137 3930 6370 11 422 259 205 139 205 17 8832 8494 579 8295 30125 579 287 579 5058 30125 579 3945 18 121 111 9155 3267 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: HYPONYM-OF (id = 6)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 30, 16\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@3::(106,111)-(114,115)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i . e . [unused16] named entity - lr ##b - ne - rr ##b - items [unused17] in the [unused12] japanese text [unused13] . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 580 6893 12951 434 2992 355 111 2538 697 168 111 3512 137 2998 131 1193 8541 422 4058 137 3930 6370 422 259 205 139 205 17 8832 8494 579 8295 30125 579 287 579 5058 30125 579 3945 18 121 111 13 9155 3267 14 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - label: PART-OF (id = 1)\n",
      "11/13/2023 21:39:48 - INFO - root - sub_idx, obj_idx: 28, 44\n",
      "11/13/2023 21:39:48 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:48 - INFO - root - guid: X96-1059@5::(125,125)-(127,128)\n",
      "11/13/2023 21:39:48 - INFO - root - tokens: [CLS] [unused33] amorph [unused34] recognizes [unused9] ne items [unused10] in two stages : dictionary lookup and rule application . [SEP]\n",
      "11/13/2023 21:39:48 - INFO - root - input_ids: 102 34 15291 35 20269 10 287 3945 11 121 502 4303 862 13050 22474 137 3346 1836 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:48 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:49 - INFO - root - sub_idx, obj_idx: 1, 5\n",
      "11/13/2023 21:39:49 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:49 - INFO - root - guid: X96-1059@5::(133,134)-(125,125)\n",
      "11/13/2023 21:39:49 - INFO - root - tokens: [CLS] [unused30] amorph [unused31] recognizes ne items in two stages : [unused33] dictionary lookup [unused34] and rule application . [SEP]\n",
      "11/13/2023 21:39:49 - INFO - root - input_ids: 102 31 15291 32 20269 287 3945 121 502 4303 862 34 13050 22474 35 137 3346 1836 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - label: PART-OF (id = 1)\n",
      "11/13/2023 21:39:49 - INFO - root - sub_idx, obj_idx: 11, 1\n",
      "11/13/2023 21:39:49 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:49 - INFO - root - guid: X96-1059@5::(133,134)-(136,137)\n",
      "11/13/2023 21:39:49 - INFO - root - tokens: [CLS] amorph recognizes ne items in two stages : [unused33] dictionary lookup [unused34] and [unused30] rule application [unused31] . [SEP]\n",
      "11/13/2023 21:39:49 - INFO - root - input_ids: 102 15291 20269 287 3945 121 502 4303 862 34 13050 22474 35 137 31 3346 1836 32 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - label: CONJUNCTION (id = 4)\n",
      "11/13/2023 21:39:49 - INFO - root - sub_idx, obj_idx: 9, 14\n",
      "11/13/2023 21:39:49 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:49 - INFO - root - guid: X96-1059@5::(136,137)-(125,125)\n",
      "11/13/2023 21:39:49 - INFO - root - tokens: [CLS] [unused30] amorph [unused31] recognizes ne items in two stages : dictionary lookup and [unused33] rule application [unused34] . [SEP]\n",
      "11/13/2023 21:39:49 - INFO - root - input_ids: 102 31 15291 32 20269 287 3945 121 502 4303 862 13050 22474 137 34 3346 1836 35 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - label: PART-OF (id = 1)\n",
      "11/13/2023 21:39:49 - INFO - root - sub_idx, obj_idx: 14, 1\n",
      "11/13/2023 21:39:49 - INFO - root - *** Example ***\n",
      "11/13/2023 21:39:49 - INFO - root - guid: X96-1059@6::(146,146)-(141,141)\n",
      "11/13/2023 21:39:49 - INFO - root - tokens: [CLS] first , [unused18] it [unused19] uses several kinds of [unused16] diction ##aries [unused17] to segment and tag japanese character strings . [SEP]\n",
      "11/13/2023 21:39:49 - INFO - root - input_ids: 102 705 422 19 256 20 3294 1323 7337 131 17 11618 2881 18 147 3197 137 5374 9155 954 13408 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/13/2023 21:39:49 - INFO - root - label: USED-FOR (id = 2)\n",
      "11/13/2023 21:39:49 - INFO - root - sub_idx, obj_idx: 10, 3\n",
      "11/13/2023 21:39:58 - INFO - root - Average #tokens: 44.54\n",
      "11/13/2023 21:39:58 - INFO - root - Max #tokens: 131\n",
      "11/13/2023 21:39:58 - INFO - root - 4906 (96.92 %) examples can fit max_seq_length = 128\n",
      "11/13/2023 21:39:58 - INFO - root - {'SUBJ_START': '[unused1]', 'SUBJ_END': '[unused2]', 'OBJ_START': '[unused3]', 'OBJ_END': '[unused4]', 'SUBJ=Generic': '[unused5]', 'OBJ=OtherScientificTerm': '[unused6]', 'SUBJ_START=Generic': '[unused7]', 'SUBJ_END=Generic': '[unused8]', 'OBJ_START=OtherScientificTerm': '[unused9]', 'OBJ_END=OtherScientificTerm': '[unused10]', 'OBJ=Material': '[unused11]', 'OBJ_START=Material': '[unused12]', 'OBJ_END=Material': '[unused13]', 'SUBJ=OtherScientificTerm': '[unused14]', 'OBJ=Generic': '[unused15]', 'SUBJ_START=OtherScientificTerm': '[unused16]', 'SUBJ_END=OtherScientificTerm': '[unused17]', 'OBJ_START=Generic': '[unused18]', 'OBJ_END=Generic': '[unused19]', 'SUBJ=Material': '[unused20]', 'SUBJ_START=Material': '[unused21]', 'SUBJ_END=Material': '[unused22]', 'OBJ=Task': '[unused23]', 'OBJ_START=Task': '[unused24]', 'OBJ_END=Task': '[unused25]', 'SUBJ=Task': '[unused26]', 'SUBJ_START=Task': '[unused27]', 'SUBJ_END=Task': '[unused28]', 'OBJ=Method': '[unused29]', 'OBJ_START=Method': '[unused30]', 'OBJ_END=Method': '[unused31]', 'SUBJ=Method': '[unused32]', 'SUBJ_START=Method': '[unused33]', 'SUBJ_END=Method': '[unused34]', 'OBJ=Metric': '[unused35]', 'OBJ_START=Metric': '[unused36]', 'OBJ_END=Metric': '[unused37]', 'SUBJ=Metric': '[unused38]', 'SUBJ_START=Metric': '[unused39]', 'SUBJ_END=Metric': '[unused40]'}\n",
      "11/13/2023 21:39:58 - INFO - root - ***** Test *****\n",
      "11/13/2023 21:39:58 - INFO - root -   Num examples = 5062\n",
      "11/13/2023 21:39:58 - INFO - root -   Batch size = 8\n",
      "11/13/2023 21:39:58 - INFO - transformers.configuration_utils - loading configuration file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/rel_approx-scib-ctx0/config.json\n",
      "11/13/2023 21:39:58 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "11/13/2023 21:39:58 - INFO - transformers.modeling_utils - loading weights file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/rel_approx-scib-ctx0/pytorch_model.bin\n",
      "11/13/2023 21:40:00 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BertForRelation.\n",
      "\n",
      "11/13/2023 21:40:00 - INFO - transformers.modeling_utils - All the weights of BertForRelation were initialized from the model checkpoint at C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/rel_approx-scib-ctx0/.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForRelation for predictions without further training.\n",
      "11/13/2023 21:43:00 - INFO - root - ***** Eval results *****\n",
      "11/13/2023 21:43:00 - INFO - root -   accuracy = 0.8557882259976294\n",
      "11/13/2023 21:43:00 - INFO - root -   eval_loss = 0.4450051788046462\n",
      "11/13/2023 21:43:00 - INFO - root -   f1 = 0.6205910390848427\n",
      "11/13/2023 21:43:00 - INFO - root -   n_correct = 651\n",
      "11/13/2023 21:43:00 - INFO - root -   n_gold = 974\n",
      "11/13/2023 21:43:00 - INFO - root -   n_pred = 1124\n",
      "11/13/2023 21:43:00 - INFO - root -   precision = 0.5791814946619217\n",
      "11/13/2023 21:43:00 - INFO - root -   recall = 0.6683778234086243\n",
      "11/13/2023 21:43:00 - INFO - root -   task_f1 = 0.6205910390848427\n",
      "11/13/2023 21:43:00 - INFO - root -   task_ngold = 974\n",
      "11/13/2023 21:43:00 - INFO - root -   task_recall = 0.6683778234086243\n",
      "11/13/2023 21:43:00 - INFO - root - *** Evaluation Results ***\n",
      "11/13/2023 21:43:00 - INFO - root -   accuracy = 0.8557882259976294\n",
      "11/13/2023 21:43:00 - INFO - root -   eval_loss = 0.4450051788046462\n",
      "11/13/2023 21:43:00 - INFO - root -   f1 = 0.6205910390848427\n",
      "11/13/2023 21:43:00 - INFO - root -   n_correct = 651\n",
      "11/13/2023 21:43:00 - INFO - root -   n_gold = 974\n",
      "11/13/2023 21:43:00 - INFO - root -   n_pred = 1124\n",
      "11/13/2023 21:43:00 - INFO - root -   precision = 0.5791814946619217\n",
      "11/13/2023 21:43:00 - INFO - root -   recall = 0.6683778234086243\n",
      "11/13/2023 21:43:00 - INFO - root -   task_f1 = 0.6205910390848427\n",
      "11/13/2023 21:43:00 - INFO - root -   task_ngold = 974\n",
      "11/13/2023 21:43:00 - INFO - root -   task_recall = 0.6683778234086243\n",
      "11/13/2023 21:43:00 - INFO - root - Output predictions to C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/rel_approx-scib-ctx0/predictions.json..\n"
     ]
    }
   ],
   "source": [
    "CLS = \"[CLS]\"\n",
    "SEP = \"[SEP]\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger('root')\n",
    "\n",
    "if 'albert' in model_name:\n",
    "    RelationModel = AlbertForRelation\n",
    "    add_new_tokens = True\n",
    "else:\n",
    "    RelationModel = BertForRelation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# train set\n",
    "if do_train:\n",
    "    train_dataset, train_examples, train_nrel = generate_relation_data(train_file, use_gold=True, context_window=context_window)\n",
    "# dev set\n",
    "if (do_eval and do_train) or (do_eval and not(eval_test)):\n",
    "    eval_dataset, eval_examples, eval_nrel = generate_relation_data(os.path.join(entity_output_dir, entity_predictions_dev), use_gold=eval_with_gold, context_window=context_window)\n",
    "# test set\n",
    "if eval_test:\n",
    "    test_dataset, test_examples, test_nrel = generate_relation_data(os.path.join(entity_output_dir, entity_predictions_test), use_gold=eval_with_gold, context_window=context_window)\n",
    "\n",
    "setseed(seed)\n",
    "\n",
    "if not do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if do_train:\n",
    "    logger.addHandler(logging.FileHandler(os.path.join(output_dir, \"train.log\"), 'w'))\n",
    "else:\n",
    "    logger.addHandler(logging.FileHandler(os.path.join(output_dir, \"eval.log\"), 'w'))\n",
    "\n",
    "# get label_list\n",
    "if os.path.exists(os.path.join(output_dir, 'label_list.json')):\n",
    "    with open(os.path.join(output_dir, 'label_list.json'), 'r') as f:\n",
    "        label_list = json.load(f)\n",
    "else:\n",
    "    label_list = [negative_label] + task_rel_labels[task]\n",
    "    with open(os.path.join(output_dir, 'label_list.json'), 'w') as f:\n",
    "        json.dump(label_list, f)\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=do_lower_case)\n",
    "if add_new_tokens:\n",
    "    add_marker_tokens(tokenizer, task_ner_labels[task])\n",
    "\n",
    "if os.path.exists(os.path.join(output_dir, 'special_tokens.json')):\n",
    "    with open(os.path.join(output_dir, 'special_tokens.json'), 'r') as f:\n",
    "        special_tokens = json.load(f)\n",
    "else:\n",
    "    special_tokens = {}\n",
    "\n",
    "if do_eval and (do_train or not(eval_test)):\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label2id, max_seq_length, tokenizer, special_tokens, unused_tokens=not(add_new_tokens))\n",
    "    logger.info(\"***** Dev *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    all_sub_idx = torch.tensor([f.sub_idx for f in eval_features], dtype=torch.long)\n",
    "    all_obj_idx = torch.tensor([f.obj_idx for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_sub_idx, all_obj_idx)\n",
    "    eval_dataloader = DataLoader(eval_data, batch_size=eval_batch_size)\n",
    "    eval_label_ids = all_label_ids\n",
    "with open(os.path.join(output_dir, 'special_tokens.json'), 'w') as f:\n",
    "    json.dump(special_tokens, f)\n",
    "\n",
    "if do_eval:\n",
    "    logger.info(special_tokens)\n",
    "    if eval_test:\n",
    "        eval_dataset = test_dataset\n",
    "        eval_examples = test_examples\n",
    "        eval_features = convert_examples_to_features(\n",
    "            test_examples, label2id, max_seq_length, tokenizer, special_tokens, unused_tokens=not(add_new_tokens))\n",
    "        eval_nrel = test_nrel\n",
    "        logger.info(special_tokens)\n",
    "        logger.info(\"***** Test *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(test_examples))\n",
    "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        all_sub_idx = torch.tensor([f.sub_idx for f in eval_features], dtype=torch.long)\n",
    "        all_obj_idx = torch.tensor([f.obj_idx for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_sub_idx, all_obj_idx)\n",
    "        eval_dataloader = DataLoader(eval_data, batch_size=eval_batch_size)\n",
    "        eval_label_ids = all_label_ids\n",
    "    model = RelationModel.from_pretrained(output_dir, num_rel_labels=num_labels)\n",
    "    model.to(device)\n",
    "    preds, result, logits = evaluate(model, device, eval_dataloader, eval_label_ids, num_labels, e2e_ngold=eval_nrel)\n",
    "\n",
    "    logger.info('*** Evaluation Results ***')\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "\n",
    "    print_pred_json(eval_dataset, eval_examples, preds, id2label, os.path.join(output_dir, prediction_file))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
