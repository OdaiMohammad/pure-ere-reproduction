{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640d415f-6e89-42e8-af0e-a65e78034864",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the entity model\n",
    "In this notebook we will run and evalute the entity nmodel proposed in the research paper [A Frustratingly Easy Approach for Entity and Relation Extraction](https://arxiv.org/pdf/2010.12812.pdf).\n",
    "\n",
    "This is a reproduction based on the instructions left by the authors in their [GitHub repo](https://github.com/princeton-nlp/PURE)\n",
    "\n",
    "**Environment information**\n",
    "\n",
    "- Windows 11\n",
    "- Python 3.6.13\n",
    "- pip 21.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba35da-92ff-4703-8d62-dd4c93a9e96f",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "Firstly, we setup our notebook by importing needed libraries and modules.\n",
    "\n",
    "And we initialize a logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5021b5c2-94c1-419a-9b7d-8a154ee74a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.nn.util import batched_index_select\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AlbertTokenizer, AlbertPreTrainedModel, AlbertModel\n",
    "from transformers import BertTokenizer, BertPreTrainedModel, BertModel\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger('root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ece49-e225-4c2a-b646-d6a863f1aabc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main classes\n",
    "The authors have implemented their system in an OOP style.\n",
    "\n",
    "### BertForEntity\n",
    "The fisrt class we'll look at is BertForEntity. It is a custom model for named entity recognition (NER) using BERT (Bidirectional Encoder Representations from Transformers) as the underlying pre-trained transformer model. The class is built on top of a pre-trained BERT mode, therefore, it extends the BertPreTrainedModel.\n",
    "\n",
    "Let's now look into the constructor, it initializes the BERT model, dropout, width embedding, and the NER classifier layers. It then calls init_weights() to initialize the model's weights.\n",
    "\n",
    "The constructor accepts takes several parameters:\n",
    "- config: BERT model configuration.\n",
    "- num_ner_labels: Number of NER labels.\n",
    "- head_hidden_dim: Hidden dimension for the feedforward layers in the NER classifier **(default: 150)**.\n",
    "- width_embedding_dim: Dimension of the width embedding **(default: 150)**.\n",
    "- max_span_length: Maximum length for a span **(default: 8)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a61e9-4541-40cf-bc8b-e2d4ac167c3a",
   "metadata": {},
   "source": [
    "We must note the setup of the model:\n",
    "\n",
    "```python\n",
    "self.ner_classifier = nn.Sequential(\n",
    "            FeedForward(input_dim=config.hidden_size * 2 + width_embedding_dim,\n",
    "                        num_layers=2,\n",
    "                        hidden_dims=head_hidden_dim,\n",
    "                        activations=F.relu,\n",
    "                        dropout=0.2),\n",
    "            nn.Linear(head_hidden_dim, num_ner_labels))\n",
    "```\n",
    "\n",
    "The model is a FeedForward Neural Network:\n",
    "\n",
    "- It's defined using nn.Sequential and consists of two layers.\n",
    "- Input dimension (input_dim) is set to the sum of the following:\n",
    "  - config.hidden_size * 2: Twice the hidden size of the BERT model. This is likely because the span embeddings are concatenated, so the input dimension includes information from both start and end embeddings.\n",
    "  - width_embedding_dim: Dimension of the width embedding.\n",
    "- num_layers=2: There are two hidden layers in the feedforward network.\n",
    "- hidden_dims=head_hidden_dim: The hidden layer dimensions are set by the head_hidden_dim parameter **(default: 150)**.\n",
    "- activations=F.relu: Rectified Linear Unit (ReLU) activation function is used between layers.\n",
    "- dropout=0.2: 20% dropout is applied between layers for regularization.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "- After the feedforward layers, there is a linear layer (nn.Linear) with an output dimension of num_ner_labels.\n",
    "- This output dimension corresponds to the number of NER labels, indicating the classes the model is trying to predict.\n",
    "\n",
    "**Note:**\n",
    "The choice of ReLU activation and dropout between layers is a common practice in neural network architectures for introducing non-linearity and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "154336b5-a29c-4672-9e32-90875cdbb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForEntity(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_ner_labels, head_hidden_dim=150, width_embedding_dim=150, max_span_length=8):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.hidden_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.width_embedding = nn.Embedding(max_span_length + 1, width_embedding_dim)\n",
    "\n",
    "        self.ner_classifier = nn.Sequential(\n",
    "            FeedForward(input_dim=config.hidden_size * 2 + width_embedding_dim,\n",
    "                        num_layers=2,\n",
    "                        hidden_dims=head_hidden_dim,\n",
    "                        activations=F.relu,\n",
    "                        dropout=0.2),\n",
    "            nn.Linear(head_hidden_dim, num_ner_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _get_span_embeddings(self, input_ids, spans, token_type_ids=None, attention_mask=None):\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids,\n",
    "                                                   attention_mask=attention_mask)\n",
    "\n",
    "        sequence_output = self.hidden_dropout(sequence_output)\n",
    "\n",
    "        \"\"\"\n",
    "        spans: [batch_size, num_spans, 3]; 0: left_ned, 1: right_end, 2: width\n",
    "        spans_mask: (batch_size, num_spans, )\n",
    "        \"\"\"\n",
    "        spans_start = spans[:, :, 0].view(spans.size(0), -1)\n",
    "        spans_start_embedding = batched_index_select(sequence_output, spans_start)\n",
    "        spans_end = spans[:, :, 1].view(spans.size(0), -1)\n",
    "        spans_end_embedding = batched_index_select(sequence_output, spans_end)\n",
    "\n",
    "        spans_width = spans[:, :, 2].view(spans.size(0), -1)\n",
    "        spans_width_embedding = self.width_embedding(spans_width)\n",
    "\n",
    "        # Concatenate embeddings of left/right points and the width embedding\n",
    "        spans_embedding = torch.cat((spans_start_embedding, spans_end_embedding, spans_width_embedding), dim=-1)\n",
    "        \"\"\"\n",
    "        spans_embedding: (batch_size, num_spans, hidden_size*2+embedding_dim)\n",
    "        \"\"\"\n",
    "        return spans_embedding\n",
    "\n",
    "    def forward(self, input_ids, spans, spans_mask, spans_ner_label=None, token_type_ids=None, attention_mask=None):\n",
    "        spans_embedding = self._get_span_embeddings(input_ids, spans, token_type_ids=token_type_ids,\n",
    "                                                    attention_mask=attention_mask)\n",
    "        ffnn_hidden = []\n",
    "        hidden = spans_embedding\n",
    "        for layer in self.ner_classifier:\n",
    "            hidden = layer(hidden)\n",
    "            ffnn_hidden.append(hidden)\n",
    "        logits = ffnn_hidden[-1]\n",
    "\n",
    "        if spans_ner_label is not None:\n",
    "            loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "            if attention_mask is not None:\n",
    "                active_loss = spans_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, logits.shape[-1])\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, spans_ner_label.view(-1), torch.tensor(loss_fct.ignore_index).type_as(spans_ner_label)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, logits.shape[-1]), spans_ner_label.view(-1))\n",
    "            return loss, logits, spans_embedding\n",
    "        else:\n",
    "            return logits, spans_embedding, spans_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5f3a2-ca7e-4445-8aa6-44c59f2d77de",
   "metadata": {},
   "source": [
    "### AlbertForEntity\n",
    "\n",
    "There's also the class AlbertForEntity. Which is very similar to the BertForEntity except it uses the AlbertModel isntead of the BErtModel as it's inderlying transformer model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46fc5f0b-7afd-4155-b3af-5975e79efa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertForEntity(AlbertPreTrainedModel):\n",
    "    def __init__(self, config, num_ner_labels, head_hidden_dim=150, width_embedding_dim=150, max_span_length=8):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.hidden_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.width_embedding = nn.Embedding(max_span_length+1, width_embedding_dim)\n",
    "        \n",
    "        self.ner_classifier = nn.Sequential(\n",
    "            FeedForward(input_dim=config.hidden_size*2+width_embedding_dim, \n",
    "                        num_layers=2,\n",
    "                        hidden_dims=head_hidden_dim,\n",
    "                        activations=F.relu,\n",
    "                        dropout=0.2),\n",
    "            nn.Linear(head_hidden_dim, num_ner_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _get_span_embeddings(self, input_ids, spans, token_type_ids=None, attention_mask=None):\n",
    "        sequence_output, pooled_output = self.albert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        sequence_output = self.hidden_dropout(sequence_output)\n",
    "\n",
    "        \"\"\"\n",
    "        spans: [batch_size, num_spans, 3]; 0: left_ned, 1: right_end, 2: width\n",
    "        spans_mask: (batch_size, num_spans, )\n",
    "        \"\"\"\n",
    "        spans_start = spans[:, :, 0].view(spans.size(0), -1)\n",
    "        spans_start_embedding = batched_index_select(sequence_output, spans_start)\n",
    "        spans_end = spans[:, :, 1].view(spans.size(0), -1)\n",
    "        spans_end_embedding = batched_index_select(sequence_output, spans_end)\n",
    "\n",
    "        spans_width = spans[:, :, 2].view(spans.size(0), -1)\n",
    "        spans_width_embedding = self.width_embedding(spans_width)\n",
    "\n",
    "        spans_embedding = torch.cat((spans_start_embedding, spans_end_embedding, spans_width_embedding), dim=-1)\n",
    "        \"\"\"\n",
    "        spans_embedding: (batch_size, num_spans, hidden_size*2+embedding_dim)\n",
    "        \"\"\"\n",
    "        return spans_embedding\n",
    "\n",
    "    def forward(self, input_ids, spans, spans_mask, spans_ner_label=None, token_type_ids=None, attention_mask=None):\n",
    "        spans_embedding = self._get_span_embeddings(input_ids, spans, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        ffnn_hidden = []\n",
    "        hidden = spans_embedding\n",
    "        for layer in self.ner_classifier:\n",
    "            hidden = layer(hidden)\n",
    "            ffnn_hidden.append(hidden)\n",
    "        logits = ffnn_hidden[-1]\n",
    "\n",
    "        if spans_ner_label is not None:\n",
    "            loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "            if attention_mask is not None:\n",
    "                active_loss = spans_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, logits.shape[-1])\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, spans_ner_label.view(-1), torch.tensor(loss_fct.ignore_index).type_as(spans_ner_label)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, logits.shape[-1]), spans_ner_label.view(-1))\n",
    "            return loss, logits, spans_embedding\n",
    "        else:\n",
    "            return logits, spans_embedding, spans_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beb8c4-f614-47d2-b105-368ee4a675be",
   "metadata": {},
   "source": [
    "### EntityModel\n",
    "\n",
    "This EntityModel class is a wrapper around a BERT or ALBERT model for Named Entity Recognition (NER) tasks. It's designed to provide a convenient interface for using BERT or ALBERT models for NER tasks, handling tokenization, model loading, device management, and batch processing.\n",
    "\n",
    "Let's break down the functionality and components of this class:\n",
    "\n",
    "#### nitialization (__init__ method):\n",
    "- Parameters:\n",
    "  - model: The name of the BERT or ALBERT model to be used.\n",
    "  - bert_model_dir: The directory where the pre-trained BERT or ALBERT model is stored.\n",
    "  - use_albert: A flag indicating whether to use ALBERT (True) or BERT (False).\n",
    "  - max_span_length: Maximum span length to be considered during tokenization.\n",
    "  - num_ner_labels: The number of Named Entity Recognition labels.\n",
    "  \n",
    "  \n",
    "- Initialization Steps:\n",
    "1. Set bert_model_name and vocab_name based on the provided parameters.\n",
    "2. If a BERT model directory is specified (bert_model_dir is not None), update bert_model_name and vocab_name accordingly.\n",
    "3. Initialize a tokenizer (AlbertTokenizer or BertTokenizer) based on the specified model name.\n",
    "4. Initialize the transformer model (AlbertForEntity or BertForEntity) with the provided parameters.\n",
    "\n",
    "\n",
    "#### Device Management (move_model_to_cuda method):\n",
    "\n",
    "\n",
    "- Functionality:\n",
    "  - Checks if a GPU (CUDA) is available.\n",
    "  - If available, moves the model to the GPU. If multiple GPUs are available, it wraps the model with **torch.nn.DataParallel** for parallel processing.\n",
    "\n",
    "\n",
    "##### Input Tensor Generation (\\_get_input_tensors method):\n",
    "- Functionality:\n",
    "  - Takes a list of tokens, spans, and spans' Named Entity Recognition (NER) labels.\n",
    "  - Tokenizes the input tokens using the tokenizer and converts them to indexed tokens.\n",
    "  - Converts span information to tensors.\n",
    "  \n",
    "  \n",
    "#### Batch Input Tensor Generation (_get_input_tensors_batch method):\n",
    "- Functionality:\n",
    "  - Takes a list of samples, where each sample contains tokens, spans, and spans' NER labels.\n",
    "  - Calls _get_input_tensors for each sample and constructs batched tensors for the entire input batch.\n",
    "  - Handles padding for both tokens and spans to create tensors of uniform shapes.\n",
    "#### Batch Execution (run_batch method):\n",
    "- Functionality:\n",
    "  - Converts the input samples to tensors using _get_input_tensors_batch.\n",
    "  - If in training mode, runs the BERT or ALBERT model in training mode and computes the NER loss.\n",
    "  - If in evaluation mode, runs the model in evaluation mode and generates predictions.\n",
    "  - Returns a dictionary containing NER loss, log-likelihoods (in training mode), predicted NER labels, NER probabilities, and last hidden states.\n",
    "  \n",
    "**Note:**\n",
    "- The class relies on external logger (logger) for logging information.\n",
    "- It assumes that the underlying BERT or ALBERT model classes (AlbertForEntity, BertForEntity) are correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37774b5e-6ac6-4147-af19-9755e638c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel:\n",
    "\n",
    "    def __init__(self, model, bert_model_dir, use_albert, max_span_length, num_ner_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        bert_model_name = model\n",
    "        vocab_name = bert_model_name\n",
    "\n",
    "        if bert_model_dir is not None:\n",
    "            bert_model_name = str(bert_model_dir) + '/'\n",
    "            # vocab_name = bert_model_name + 'vocab.txt'\n",
    "            vocab_name = bert_model_name\n",
    "            logger.info('Loading BERT model from {}'.format(bert_model_name))\n",
    "\n",
    "        if use_albert:\n",
    "            self.tokenizer = AlbertTokenizer.from_pretrained(vocab_name)\n",
    "            self.bert_model = AlbertForEntity.from_pretrained(bert_model_name, num_ner_labels=num_ner_labels,\n",
    "                                                              max_span_length=max_span_length)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(vocab_name)\n",
    "            self.bert_model = BertForEntity.from_pretrained(bert_model_name, num_ner_labels=num_ner_labels,\n",
    "                                                            max_span_length=max_span_length)\n",
    "\n",
    "        self._model_device = 'cpu'\n",
    "        self.move_model_to_cuda()\n",
    "\n",
    "    def move_model_to_cuda(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            logger.error('No CUDA found!')\n",
    "            exit(-1)\n",
    "        logger.info('Moving to CUDA...')\n",
    "        self._model_device = 'cuda'\n",
    "        self.bert_model.cuda()\n",
    "        logger.info('# GPUs = %d' % (torch.cuda.device_count()))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.bert_model = torch.nn.DataParallel(self.bert_model)\n",
    "\n",
    "    def _get_input_tensors(self, tokens, spans, spans_ner_label):\n",
    "        start2idx = []\n",
    "        end2idx = []\n",
    "\n",
    "        bert_tokens = []\n",
    "        bert_tokens.append(self.tokenizer.cls_token)\n",
    "        for token in tokens:\n",
    "            start2idx.append(len(bert_tokens))\n",
    "            sub_tokens = self.tokenizer.tokenize(token)\n",
    "            bert_tokens += sub_tokens\n",
    "            end2idx.append(len(bert_tokens) - 1)\n",
    "        bert_tokens.append(self.tokenizer.sep_token)\n",
    "\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "        bert_spans = [[start2idx[span[0]], end2idx[span[1]], span[2]] for span in spans]\n",
    "        bert_spans_tensor = torch.tensor([bert_spans])\n",
    "\n",
    "        spans_ner_label_tensor = torch.tensor([spans_ner_label])\n",
    "\n",
    "        return tokens_tensor, bert_spans_tensor, spans_ner_label_tensor\n",
    "\n",
    "    def _get_input_tensors_batch(self, samples_list, training=True):\n",
    "        tokens_tensor_list = []\n",
    "        bert_spans_tensor_list = []\n",
    "        spans_ner_label_tensor_list = []\n",
    "        sentence_length = []\n",
    "\n",
    "        max_tokens = 0\n",
    "        max_spans = 0\n",
    "        for sample in samples_list:\n",
    "            tokens = sample['tokens']\n",
    "            spans = sample['spans']\n",
    "            spans_ner_label = sample['spans_label']\n",
    "\n",
    "            tokens_tensor, bert_spans_tensor, spans_ner_label_tensor = self._get_input_tensors(tokens, spans,\n",
    "                                                                                               spans_ner_label)\n",
    "            tokens_tensor_list.append(tokens_tensor)\n",
    "            bert_spans_tensor_list.append(bert_spans_tensor)\n",
    "            spans_ner_label_tensor_list.append(spans_ner_label_tensor)\n",
    "            assert (bert_spans_tensor.shape[1] == spans_ner_label_tensor.shape[1])\n",
    "            if (tokens_tensor.shape[1] > max_tokens):\n",
    "                max_tokens = tokens_tensor.shape[1]\n",
    "            if (bert_spans_tensor.shape[1] > max_spans):\n",
    "                max_spans = bert_spans_tensor.shape[1]\n",
    "            sentence_length.append(sample['sent_length'])\n",
    "        sentence_length = torch.Tensor(sentence_length)\n",
    "\n",
    "        # apply padding and concatenate tensors\n",
    "        final_tokens_tensor = None\n",
    "        final_attention_mask = None\n",
    "        final_bert_spans_tensor = None\n",
    "        final_spans_ner_label_tensor = None\n",
    "        final_spans_mask_tensor = None\n",
    "        for tokens_tensor, bert_spans_tensor, spans_ner_label_tensor in zip(tokens_tensor_list, bert_spans_tensor_list,\n",
    "                                                                            spans_ner_label_tensor_list):\n",
    "            # padding for tokens\n",
    "            num_tokens = tokens_tensor.shape[1]\n",
    "            tokens_pad_length = max_tokens - num_tokens\n",
    "            attention_tensor = torch.full([1, num_tokens], 1, dtype=torch.long)\n",
    "            if tokens_pad_length > 0:\n",
    "                pad = torch.full([1, tokens_pad_length], self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "                tokens_tensor = torch.cat((tokens_tensor, pad), dim=1)\n",
    "                attention_pad = torch.full([1, tokens_pad_length], 0, dtype=torch.long)\n",
    "                attention_tensor = torch.cat((attention_tensor, attention_pad), dim=1)\n",
    "\n",
    "            # padding for spans\n",
    "            num_spans = bert_spans_tensor.shape[1]\n",
    "            spans_pad_length = max_spans - num_spans\n",
    "            spans_mask_tensor = torch.full([1, num_spans], 1, dtype=torch.long)\n",
    "            if spans_pad_length > 0:\n",
    "                pad = torch.full([1, spans_pad_length, bert_spans_tensor.shape[2]], 0, dtype=torch.long)\n",
    "                bert_spans_tensor = torch.cat((bert_spans_tensor, pad), dim=1)\n",
    "                mask_pad = torch.full([1, spans_pad_length], 0, dtype=torch.long)\n",
    "                spans_mask_tensor = torch.cat((spans_mask_tensor, mask_pad), dim=1)\n",
    "                spans_ner_label_tensor = torch.cat((spans_ner_label_tensor, mask_pad), dim=1)\n",
    "\n",
    "            # update final outputs\n",
    "            if final_tokens_tensor is None:\n",
    "                final_tokens_tensor = tokens_tensor\n",
    "                final_attention_mask = attention_tensor\n",
    "                final_bert_spans_tensor = bert_spans_tensor\n",
    "                final_spans_ner_label_tensor = spans_ner_label_tensor\n",
    "                final_spans_mask_tensor = spans_mask_tensor\n",
    "            else:\n",
    "                final_tokens_tensor = torch.cat((final_tokens_tensor, tokens_tensor), dim=0)\n",
    "                final_attention_mask = torch.cat((final_attention_mask, attention_tensor), dim=0)\n",
    "                final_bert_spans_tensor = torch.cat((final_bert_spans_tensor, bert_spans_tensor), dim=0)\n",
    "                final_spans_ner_label_tensor = torch.cat((final_spans_ner_label_tensor, spans_ner_label_tensor), dim=0)\n",
    "                final_spans_mask_tensor = torch.cat((final_spans_mask_tensor, spans_mask_tensor), dim=0)\n",
    "        # logger.info(final_tokens_tensor)\n",
    "        # logger.info(final_attention_mask)\n",
    "        # logger.info(final_bert_spans_tensor)\n",
    "        # logger.info(final_bert_spans_tensor.shape)\n",
    "        # logger.info(final_spans_mask_tensor.shape)\n",
    "        # logger.info(final_spans_ner_label_tensor.shape)\n",
    "        return final_tokens_tensor, final_attention_mask, final_bert_spans_tensor, final_spans_mask_tensor, final_spans_ner_label_tensor, sentence_length\n",
    "\n",
    "    def run_batch(self, samples_list, try_cuda=True, training=True):\n",
    "        # convert samples to input tensors\n",
    "        tokens_tensor, attention_mask_tensor, bert_spans_tensor, spans_mask_tensor, spans_ner_label_tensor, sentence_length = self._get_input_tensors_batch(\n",
    "            samples_list, training)\n",
    "\n",
    "        output_dict = {\n",
    "            'ner_loss': 0,\n",
    "        }\n",
    "\n",
    "        if training:\n",
    "            self.bert_model.train()\n",
    "            ner_loss, ner_logits, spans_embedding = self.bert_model(\n",
    "                input_ids=tokens_tensor.to(self._model_device),\n",
    "                spans=bert_spans_tensor.to(self._model_device),\n",
    "                spans_mask=spans_mask_tensor.to(self._model_device),\n",
    "                spans_ner_label=spans_ner_label_tensor.to(self._model_device),\n",
    "                attention_mask=attention_mask_tensor.to(self._model_device),\n",
    "            )\n",
    "            output_dict['ner_loss'] = ner_loss.sum()\n",
    "            output_dict['ner_llh'] = F.log_softmax(ner_logits, dim=-1)\n",
    "        else:\n",
    "            self.bert_model.eval()\n",
    "            with torch.no_grad():\n",
    "                ner_logits, spans_embedding, last_hidden = self.bert_model(\n",
    "                    input_ids=tokens_tensor.to(self._model_device),\n",
    "                    spans=bert_spans_tensor.to(self._model_device),\n",
    "                    spans_mask=spans_mask_tensor.to(self._model_device),\n",
    "                    spans_ner_label=None,\n",
    "                    attention_mask=attention_mask_tensor.to(self._model_device),\n",
    "                )\n",
    "            _, predicted_label = ner_logits.max(2)\n",
    "            predicted_label = predicted_label.cpu().numpy()\n",
    "            last_hidden = last_hidden.cpu().numpy()\n",
    "\n",
    "            predicted = []\n",
    "            pred_prob = []\n",
    "            hidden = []\n",
    "            for i, sample in enumerate(samples_list):\n",
    "                ner = []\n",
    "                prob = []\n",
    "                lh = []\n",
    "                for j in range(len(sample['spans'])):\n",
    "                    ner.append(predicted_label[i][j])\n",
    "                    # prob.append(F.softmax(ner_logits[i][j], dim=-1).cpu().numpy())\n",
    "                    prob.append(ner_logits[i][j].cpu().numpy())\n",
    "                    lh.append(last_hidden[i][j])\n",
    "                predicted.append(ner)\n",
    "                pred_prob.append(prob)\n",
    "                hidden.append(lh)\n",
    "            output_dict['pred_ner'] = predicted\n",
    "            output_dict['ner_probs'] = pred_prob\n",
    "            output_dict['ner_last_hidden'] = hidden\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cf997-a515-41b4-95b2-f106c2527047",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Represents a dataset. It's a convenient wrapper for handling datasets, reading data from JSON files, and creating Document objects.\n",
    "\n",
    "Let's break down its components:\n",
    "#### `__init__` method:\n",
    "\n",
    "- Initializes an instance of the Dataset class.\n",
    "- Takes three parameters: `json_file`, `pred_file` **(default is `None`)**, and `doc_range` **(default is `None`)**.\n",
    "- Reads data from the specified JSON files (`json_file` and `pred_file`).\n",
    "- If a document range (`doc_range`) is provided, it selects a subset of documents within that range.\n",
    "- Creates a list of Document objects based on the read data.\n",
    "\n",
    "#### `update_from_js` method:\n",
    "\n",
    "- Updates the dataset with new data (`js`).\n",
    "- Re-creates the list of `Document` objects based on the updated data.\n",
    "\n",
    "#### \\`_read method`:\n",
    "\n",
    "- Reads data from JSON files (`json_file` and optionally `pred_file`).\n",
    "If `pred_file` is provided, it merges the data from the gold (`gold_docs`) and predicted (`pred_docs`) files.\n",
    "- Returns the merged list of documents.\n",
    "\n",
    "#### `__getitem__` method:\n",
    "\n",
    "- Enables indexing of the dataset. Given an index `ix`, it returns the corresponding Document object.\n",
    "\n",
    "#### `__len__` method:\n",
    "\n",
    "- Returns the length of the dataset, i.e., the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f661a0aa-e6d6-4e06-8eda-0264e2e3708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, json_file, pred_file=None, doc_range=None):\n",
    "        self.js = self._read(json_file, pred_file)\n",
    "        if doc_range is not None:\n",
    "            self.js = self.js[doc_range[0]:doc_range[1]]\n",
    "        self.documents = [Document(js) for js in self.js]\n",
    "\n",
    "    def update_from_js(self, js):\n",
    "        self.js = js\n",
    "        self.documents = [Document(js) for js in self.js]\n",
    "\n",
    "    def _read(self, json_file, pred_file=None):\n",
    "        gold_docs = [json.loads(line) for line in open(json_file)]\n",
    "        if pred_file is None:\n",
    "            return gold_docs\n",
    "\n",
    "        pred_docs = [json.loads(line) for line in open(pred_file)]\n",
    "        merged_docs = []\n",
    "        for gold, pred in zip(gold_docs, pred_docs):\n",
    "            assert gold[\"doc_key\"] == pred[\"doc_key\"]\n",
    "            assert gold[\"sentences\"] == pred[\"sentences\"]\n",
    "            merged = copy.deepcopy(gold)\n",
    "            for k, v in pred.items():\n",
    "                if \"predicted\" in k:\n",
    "                    merged[k] = v\n",
    "            merged_docs.append(merged)\n",
    "\n",
    "        return merged_docs\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.documents[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d6a61-eb95-47a4-afe8-ec90a40f8cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Document\n",
    "\n",
    "This class represents a document. It encapsulates information about a document, its sentences, and any associated clusters. It provides methods for accessing and manipulating this information.\n",
    "\n",
    "Let's go through each part:\n",
    "\n",
    "#### `__init__` method:\n",
    "\n",
    "- Initializes an instance of the Document class.\n",
    "- Takes a JSON object (`js`) as input.\n",
    "- Extracts the document key (`_doc_key`) from the JSON object.\n",
    "- Uses the `fields_to_batches` function to extract specific fields from the JSON and create a list of entries.\n",
    "- Computes sentence lengths and starts to facilitate sentence indexing.\n",
    "- Creates a list of Sentence objects based on the entries.\n",
    "- If \"`clusters`\" or \"`predicted_clusters`\" are present in the JSON, creates lists of Cluster objects for clusters and predicted clusters.\n",
    "\n",
    "#### `__repr__` method:\n",
    "\n",
    "- Returns a string representation of the document, including sentence indices and their corresponding text.\n",
    "\n",
    "#### `__getitem__` method:\n",
    "\n",
    "- Enables indexing of the document. Given an index ix, it returns the corresponding `Sentence` object.\n",
    "\n",
    "#### `__len__` method:\n",
    "\n",
    "- Returns the number of sentences in the document.\n",
    "\n",
    "#### `print_plaintext method`:\n",
    "\n",
    "- Prints the plaintext representation of the document, sentence by sentence.\n",
    "\n",
    "#### `find_cluster` method:\n",
    "\n",
    "- Searches through reference clusters (either predicted or actual) to find the one containing a specified entity.\n",
    "- Returns the found cluster or None if no match is found.\n",
    "\n",
    "#### `n_tokens property`:\n",
    "\n",
    "- Returns the total number of tokens in the document by summing the number of tokens in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f8414bdc-9404-4a84-8196-1d3329cda851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, js):\n",
    "        self._doc_key = js[\"doc_key\"]\n",
    "        entries = fields_to_batches(js, [\"doc_key\", \"clusters\", \"predicted_clusters\", \"section_starts\"])\n",
    "        sentence_lengths = [len(entry[\"sentences\"]) for entry in entries]\n",
    "        sentence_starts = np.cumsum(sentence_lengths)\n",
    "        sentence_starts = np.roll(sentence_starts, 1)\n",
    "        sentence_starts[0] = 0\n",
    "        self.sentence_starts = sentence_starts\n",
    "        self.sentences = [Sentence(entry, sentence_start, sentence_ix)\n",
    "                          for sentence_ix, (entry, sentence_start)\n",
    "                          in enumerate(zip(entries, sentence_starts))]\n",
    "        if \"clusters\" in js:\n",
    "            self.clusters = [Cluster(entry, i, self)\n",
    "                             for i, entry in enumerate(js[\"clusters\"])]\n",
    "        if \"predicted_clusters\" in js:\n",
    "            self.predicted_clusters = [Cluster(entry, i, self)\n",
    "                                       for i, entry in enumerate(js[\"predicted_clusters\"])]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([str(i) + \": \" + \" \".join(sent.text) for i, sent in enumerate(self.sentences)])\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def print_plaintext(self):\n",
    "        for sent in self:\n",
    "            print(\" \".join(sent.text))\n",
    "\n",
    "\n",
    "    def find_cluster(self, entity, predicted=True):\n",
    "        \"\"\"\n",
    "        Search through erence clusters and return the one containing the query entity, if it's\n",
    "        part of a cluster. If we don't find a match, return None.\n",
    "        \"\"\"\n",
    "        clusters = self.predicted_clusters if predicted else self.clusters\n",
    "        for clust in clusters:\n",
    "            for entry in clust:\n",
    "                if entry.span == entity.span:\n",
    "                    return clust\n",
    "\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return sum([len(sent) for sent in self.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516acbd4-ae1a-4aad-801c-6f53e9b163a2",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "The `Cluster` class represents a cluster of entities within a document. It is used to group together entities that belong to the same cluster, providing information about the members of the cluster.\n",
    "This class is designed to encapsulate information about a cluster of entities within a document. It facilitates the organization and representation of entities belonging to the same cluster.\n",
    "\n",
    "#### Initialization\n",
    "- Parameters:\n",
    "  - `cluster`: A list of entries representing the entities in the cluster.\n",
    "  - `cluster_id`: An identifier for the cluster.\n",
    "  - `document`: The document object to which the cluster belongs.\n",
    "- Attributes\n",
    "  - `members`: A list of ClusterMember instances, each representing an entity in the cluster.\n",
    "  - `cluster_id`: The identifier for the cluster.\n",
    "  \n",
    "#### Initialization Details\n",
    "- The `__init__` method initializes the cluster by extracting information about each entity in the cluster.\n",
    "- For each entry in the cluster, it determines the corresponding sentence and span in the document.\n",
    "- It creates `ClusterMember` instances for each entity and appends them to the members list.\n",
    "\n",
    "#### Representation\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the cluster, including the cluster identifier and a representation of its members.\n",
    "Accessing Members\n",
    "\n",
    "`__getitem__` Method:\n",
    "Allows accessing individual members of the cluster using indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b147c0b-05c7-48ab-916e-d290af305a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, cluster, cluster_id, document):\n",
    "        members = []\n",
    "        for entry in cluster:\n",
    "            sentence_ix = get_sentence_of_span(entry, document.sentence_starts, document.n_tokens)\n",
    "            sentence = document[sentence_ix]\n",
    "            span = Span(entry[0], entry[1], sentence.text, sentence.sentence_start)\n",
    "            ners = [x for x in sentence.ner if x.span == span]\n",
    "            assert len(ners) <= 1\n",
    "            ner = ners[0] if len(ners) == 1 else None\n",
    "            to_append = ClusterMember(span, ner, sentence, cluster_id)\n",
    "            members.append(to_append)\n",
    "\n",
    "        self.members = members\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.cluster_id}: \" + self.members.__repr__()\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.members[ix]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d268f6-226f-4e58-8d15-850a20aab354",
   "metadata": {},
   "source": [
    "### ClusterMember\n",
    "\n",
    "Represents an individual entity within a cluster. It provides information about the entity's span, associated named entity recognition (NER) information, the sentence it belongs to, and the identifier of the cluster to which it is assigned. The class serves as a container for information about an individual entity within a cluster. It encapsulates details such as the span, NER information, sentence context, and the cluster to which the entity is assigned.\n",
    "\n",
    "#### Initialization\n",
    "- Parameters:\n",
    "  - `span`: A Span instance representing the span of the entity in the document.\n",
    "  - `ner`: A NER instance representing the NER information for the entity.\n",
    "  - `sentence`: A Sentence instance representing the sentence containing the entity.\n",
    "  - `cluster_id`: The identifier of the cluster to which the entity belongs.\n",
    "- Attributes\n",
    "  - `span`: A Span instance representing the span of the entity.\n",
    "  - `ner`: A NER instance representing the NER information for the entity.\n",
    "  - `sentence`: A Sentence instance representing the sentence containing the entity.\n",
    "  - `cluster_id`: The identifier of the cluster to which the entity belongs.\n",
    "  \n",
    "#### Initialization Details\n",
    "\n",
    "The `__init__` method initializes a `ClusterMember` by assigning values to its attributes based on the provided parameters.\n",
    "Representation\n",
    "\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the `ClusterMember`, including the sentence index and a representation of its span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79298118-30aa-4827-8b14-9f04d1e27fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterMember:\n",
    "    def __init__(self, span, ner, sentence, cluster_id):\n",
    "        self.span = span\n",
    "        self.ner = ner\n",
    "        self.sentence = sentence\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.sentence.sentence_ix}> \" + self.span.__repr__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d4353-5d76-4aec-a46c-097943a25f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sentence\n",
    "\n",
    "This class represents a sentence. It encapsulates information about a sentence, including its text and associated entities (NER, relations, events). It provides methods for accessing and manipulating this information.\n",
    "\n",
    "Let's go through each method:\n",
    "\n",
    "#### `__init__` method:\n",
    "\n",
    "- Initializes an instance of the `Sentence` class.\n",
    "- Takes an `entry`, `sentence_start` (index of the sentence start in the document), and `sentence_ix` (sentence index) as input.\n",
    "- Stores information about the sentence's start position, text, and index.\n",
    "- Parses gold entities (NER, relations, events) and predicted entities (NER, relations, events).\n",
    "- Stores top spans if available.\n",
    "\n",
    "#### `__repr__` method:\n",
    "\n",
    "- Returns a string representation of the sentence, including the text and token indices.\n",
    "\n",
    "#### `__len__` method:\n",
    "\n",
    "-Returns the number of tokens in the sentence.\n",
    "\n",
    "#### `get_flavor` method:\n",
    "\n",
    "- Given an argument (presumably an entity), retrieves its flavor from the gold NER annotations.\n",
    "- If multiple NER annotations are found for the same span, prints a message (debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89456192-84e8-4cf3-8833-605db4f7481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, entry, sentence_start, sentence_ix):\n",
    "        self.sentence_start = sentence_start\n",
    "        self.text = entry[\"sentences\"]\n",
    "        self.sentence_ix = sentence_ix\n",
    "        # Gold\n",
    "        if \"ner_flavor\" in entry:\n",
    "            self.ner = [NER(this_ner, self.text, sentence_start, flavor=this_flavor)\n",
    "                        for this_ner, this_flavor in zip(entry[\"ner\"], entry[\"ner_flavor\"])]\n",
    "        elif \"ner\" in entry:\n",
    "            self.ner = [NER(this_ner, self.text, sentence_start)\n",
    "                        for this_ner in entry[\"ner\"]]\n",
    "        if \"relations\" in entry:\n",
    "            self.relations = [Relation(this_relation, self.text, sentence_start) for\n",
    "                              this_relation in entry[\"relations\"]]\n",
    "        if \"events\" in entry:\n",
    "            self.events = Events(entry[\"events\"], self.text, sentence_start)\n",
    "\n",
    "        # Predicted\n",
    "        if \"predicted_ner\" in entry:\n",
    "            self.predicted_ner = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
    "                                  this_ner in entry[\"predicted_ner\"]]\n",
    "        if \"predicted_relations\" in entry:\n",
    "            self.predicted_relations = [Relation(this_relation, self.text, sentence_start) for\n",
    "                                        this_relation in entry[\"predicted_relations\"]]\n",
    "        if \"predicted_events\" in entry:\n",
    "            self.predicted_events = Events(entry[\"predicted_events\"], self.text, sentence_start)\n",
    "\n",
    "        # Top spans\n",
    "        if \"top_spans\" in entry:\n",
    "            self.top_spans = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
    "                                this_ner in entry[\"top_spans\"]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        the_text = \" \".join(self.text)\n",
    "        the_lengths = np.array([len(x) for x in self.text])\n",
    "        tok_ixs = \"\"\n",
    "        for i, offset in enumerate(the_lengths):\n",
    "            true_offset = offset if i < 10 else offset - 1\n",
    "            tok_ixs += str(i)\n",
    "            tok_ixs += \" \" * true_offset\n",
    "\n",
    "        return the_text + \"\\n\" + tok_ixs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def get_flavor(self, argument):\n",
    "        the_ner = [x for x in self.ner if x.span == argument.span]\n",
    "        if len(the_ner) > 1:\n",
    "            print(\"Weird\")\n",
    "        if the_ner:\n",
    "            the_flavor = the_ner[0].flavor\n",
    "        else:\n",
    "            the_flavor = None\n",
    "        return the_flavor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b54ec5-76bf-42d5-be74-54158903f5cf",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "The NER class represents a Named Entity Recognition annotation within a sentence. It encapsulates information about a named entity, providing methods for representation and equality checks. The `Span` (we'll get to it next) class is used to represent the span of the named entity within the context of the sentence.\n",
    "\n",
    "Here are some details about it:\n",
    "#### Initialization\n",
    "\n",
    "- Parameters:\n",
    "  - `ner`: A list containing information about the NER span, including start index, end index, and label.\n",
    "  - `text`: The text content of the sentence.\n",
    "  - `sentence_start`: The index of the sentence start in the document.\n",
    "  - `flavor`: An optional parameter representing the flavor or type of the named entity.\n",
    "  \n",
    "- Attributes:\n",
    "  - `span`: An instance of the Span class representing the span of the NER annotation.\n",
    "  - `label`: The label assigned to the NER entity.\n",
    "  - `flavor`: The flavor or type of the named entity.\n",
    "  \n",
    "- Representation\n",
    "\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the NER annotation, including the span and label.\n",
    "Equality Check\n",
    "\n",
    "`__eq__` Method:\n",
    "Checks if two NER instances are equal by comparing their span, label, and flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ffb706e4-8d08-42c1-99f5-9dce04b6b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER:\n",
    "    def __init__(self, ner, text, sentence_start, flavor=None):\n",
    "        self.span = Span(ner[0], ner[1], text, sentence_start)\n",
    "        self.label = ner[2]\n",
    "        self.flavor = flavor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.span.__repr__() + \": \" + self.label\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.span == other.span and\n",
    "                self.label == other.label and\n",
    "                self.flavor == other.flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b324b-fad6-45c0-9998-64e92381e67c",
   "metadata": {},
   "source": [
    "### Span\n",
    "\n",
    "The `Span` class represents a span of text within a document. It encapsulate information about a text span, providing methods for representation, equality checks, and hashing. It ensures the proper handling of spans within the document and sentence contexts.\n",
    "\n",
    "#### Initialization\n",
    "- Parameters:\n",
    "  - `start`: The starting index of the span in the entire document.\n",
    "  - `end`: The ending index of the span in the entire document.\n",
    "  - `text`: The text content of the entire document.\n",
    "  - `sentence_start`: The index of the sentence start in the document.\n",
    "  \n",
    "- Attributes\n",
    "  - `start_doc`, `end_doc`: The start and end indices of the span in the entire document.\n",
    "  - `span_doc`: A tuple representing the span in the entire document.\n",
    "  - `start_sent`, `end_sent`: The start and end indices of the span within the sentence.\n",
    "  - `span_sent`: A tuple representing the span within the sentence.\n",
    "  - `text`: The actual text content of the span.\n",
    "  \n",
    "#### Representation\n",
    "\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the span, including start and end indices and the actual text content.\n",
    "Equality Check\n",
    "\n",
    "`__eq__` Method:\n",
    "Checks if two Span instances are equal by comparing their spans in both the document and the sentence, along with the text content.\n",
    "Hashing\n",
    "\n",
    "`__hash__` Method:\n",
    "Computes a hash value for the Span instance based on its document and sentence spans, as well as the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3c06fb6-8119-4258-a449-29cf2cdf0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, start, end, text, sentence_start):\n",
    "        self.start_doc = start\n",
    "        self.end_doc = end\n",
    "        self.span_doc = (self.start_doc, self.end_doc)\n",
    "        self.start_sent = start - sentence_start\n",
    "        self.end_sent = end - sentence_start\n",
    "        self.span_sent = (self.start_sent, self.end_sent)\n",
    "        self.text = text[self.start_sent:self.end_sent + 1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str((self.start_sent, self.end_sent, self.text))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.span_doc == other.span_doc and\n",
    "                self.span_sent == other.span_sent and\n",
    "                self.text == other.text)\n",
    "\n",
    "    def __hash__(self):\n",
    "        tup = self.span_doc + self.span_sent + (\" \".join(self.text),)\n",
    "        return hash(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d996f9-66a7-40ba-b5ce-ffff20169bad",
   "metadata": {},
   "source": [
    "### NpEncoder\n",
    "This class is a custom JSON encoder that extends the json.JSONEncoder class. It is designed to handle the encoding of NumPy-specific data types into a JSON-compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "105c3337-8efb-41f5-8469-3283ed350c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f51af1-6622-483e-bcec-44060540a4c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "361483ca-d980-47b2-a512-89d43bd5ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fields_to_batches(d, keys_to_ignore=[]):\n",
    "    keys = [key for key in d.keys() if key not in keys_to_ignore]\n",
    "    lengths = [len(d[k]) for k in keys]\n",
    "    assert len(set(lengths)) == 1\n",
    "    length = lengths[0]\n",
    "    res = [{k: d[k][i] for k in keys} for i in range(length)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e485a789-2c92-4840-aa83-c4534fe397fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_of_span(span, sentence_starts, doc_tokens):\n",
    "    \"\"\"\n",
    "    Return the index of the sentence that the span is part of.\n",
    "    \"\"\"\n",
    "    # Inclusive sentence ends\n",
    "    sentence_ends = [x - 1 for x in sentence_starts[1:]] + [doc_tokens - 1]\n",
    "    in_between = [span[0] >= start and span[1] <= end\n",
    "                  for start, end in zip(sentence_starts, sentence_ends)]\n",
    "    assert sum(in_between) == 1\n",
    "    the_sentence = in_between.index(True)\n",
    "    return the_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ed493d5-63d1-4163-91d5-b121f747044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_samples(dataset, max_span_length, ner_label2id=None, context_window=0, split=0):\n",
    "    \"\"\"\n",
    "    Extract sentences and gold entities from a dataset\n",
    "    \"\"\"\n",
    "    # split: split the data into train and dev (for ACE04)\n",
    "    # split == 0: don't split\n",
    "    # split == 1: return first 90% (train)\n",
    "    # split == 2: return last 10% (dev)\n",
    "    samples = []\n",
    "    num_ner = 0\n",
    "    max_len = 0\n",
    "    max_ner = 0\n",
    "    num_overlap = 0\n",
    "    \n",
    "    if split == 0:\n",
    "        data_range = (0, len(dataset))\n",
    "    elif split == 1:\n",
    "        data_range = (0, int(len(dataset)*0.9))\n",
    "    elif split == 2:\n",
    "        data_range = (int(len(dataset)*0.9), len(dataset))\n",
    "\n",
    "    for c, doc in enumerate(dataset):\n",
    "        if c < data_range[0] or c >= data_range[1]:\n",
    "            continue\n",
    "        for i, sent in enumerate(doc):\n",
    "            num_ner += len(sent.ner)\n",
    "            sample = {\n",
    "                'doc_key': doc._doc_key,\n",
    "                'sentence_ix': sent.sentence_ix,\n",
    "            }\n",
    "            if context_window != 0 and len(sent.text) > context_window:\n",
    "                logger.info('Long sentence: {} {}'.format(sample, len(sent.text)))\n",
    "                # print('Exclude:', sample)\n",
    "                # continue\n",
    "            sample['tokens'] = sent.text\n",
    "            sample['sent_length'] = len(sent.text)\n",
    "            sent_start = 0\n",
    "            sent_end = len(sample['tokens'])\n",
    "\n",
    "            max_len = max(max_len, len(sent.text))\n",
    "            max_ner = max(max_ner, len(sent.ner))\n",
    "\n",
    "            if context_window > 0:\n",
    "                add_left = (context_window-len(sent.text)) // 2\n",
    "                add_right = (context_window-len(sent.text)) - add_left\n",
    "                \n",
    "                # add left context\n",
    "                j = i - 1\n",
    "                while j >= 0 and add_left > 0:\n",
    "                    context_to_add = doc[j].text[-add_left:]\n",
    "                    sample['tokens'] = context_to_add + sample['tokens']\n",
    "                    add_left -= len(context_to_add)\n",
    "                    sent_start += len(context_to_add)\n",
    "                    sent_end += len(context_to_add)\n",
    "                    j -= 1\n",
    "\n",
    "                # add right context\n",
    "                j = i + 1\n",
    "                while j < len(doc) and add_right > 0:\n",
    "                    context_to_add = doc[j].text[:add_right]\n",
    "                    sample['tokens'] = sample['tokens'] + context_to_add\n",
    "                    add_right -= len(context_to_add)\n",
    "                    j += 1\n",
    "\n",
    "            sample['sent_start'] = sent_start\n",
    "            sample['sent_end'] = sent_end\n",
    "            sample['sent_start_in_doc'] = sent.sentence_start\n",
    "            \n",
    "            sent_ner = {}\n",
    "            for ner in sent.ner:\n",
    "                sent_ner[ner.span.span_sent] = ner.label\n",
    "\n",
    "            span2id = {}\n",
    "            sample['spans'] = []\n",
    "            sample['spans_label'] = []\n",
    "            for i in range(len(sent.text)):\n",
    "                for j in range(i, min(len(sent.text), i+max_span_length)):\n",
    "                    sample['spans'].append((i+sent_start, j+sent_start, j-i+1))\n",
    "                    span2id[(i, j)] = len(sample['spans'])-1\n",
    "                    if (i, j) not in sent_ner:\n",
    "                        sample['spans_label'].append(0)\n",
    "                    else:\n",
    "                        sample['spans_label'].append(ner_label2id[sent_ner[(i, j)]])\n",
    "            samples.append(sample)\n",
    "    avg_length = sum([len(sample['tokens']) for sample in samples]) / len(samples)\n",
    "    max_length = max([len(sample['tokens']) for sample in samples])\n",
    "    logger.info('# Overlap: %d'%num_overlap)\n",
    "    logger.info('Extracted %d samples from %d documents, with %d NER labels, %.3f avg input length, %d max length'%(len(samples), data_range[1]-data_range[0], num_ner, avg_length, max_length))\n",
    "    logger.info('Max Length: %d, max NER: %d'%(max_len, max_ner))\n",
    "    return samples, num_ner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99a2301a-2a86-422e-a00e-300aab275ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(samples, batch_size):\n",
    "    \"\"\"\n",
    "    Batchfy samples with a batch size\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "\n",
    "    list_samples_batches = []\n",
    "    \n",
    "    # if a sentence is too long, make itself a batch to avoid GPU OOM\n",
    "    to_single_batch = []\n",
    "    for i in range(0, len(samples)):\n",
    "        if len(samples[i]['tokens']) > 350:\n",
    "            to_single_batch.append(i)\n",
    "    \n",
    "    for i in to_single_batch:\n",
    "        logger.info('Single batch sample: %s-%d', samples[i]['doc_key'], samples[i]['sentence_ix'])\n",
    "        list_samples_batches.append([samples[i]])\n",
    "    samples = [sample for i, sample in enumerate(samples) if i not in to_single_batch]\n",
    "\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        list_samples_batches.append(samples[i:i+batch_size])\n",
    "\n",
    "    assert(sum([len(batch) for batch in list_samples_batches]) == num_samples)\n",
    "\n",
    "    return list_samples_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d2737e8-1fd1-4d4a-a5be-cf3c80b336c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelmap(label_list):\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    for i, label in enumerate(label_list):\n",
    "        label2id[label] = i + 1\n",
    "        id2label[i + 1] = label\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b146633-7da1-4afb-9ad5-77d7f4fd9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batches, tot_gold):\n",
    "    \"\"\"\n",
    "    Evaluate the entity model\n",
    "    \"\"\"\n",
    "    logger.info('Evaluating...')\n",
    "    c_time = time.time()\n",
    "    cor = 0\n",
    "    tot_pred = 0\n",
    "    l_cor = 0\n",
    "    l_tot = 0\n",
    "\n",
    "    for i in range(len(batches)):\n",
    "        output_dict = model.run_batch(batches[i], training=False)\n",
    "        pred_ner = output_dict['pred_ner']\n",
    "        for sample, preds in zip(batches[i], pred_ner):\n",
    "            for gold, pred in zip(sample['spans_label'], preds):\n",
    "                l_tot += 1\n",
    "                if pred == gold:\n",
    "                    l_cor += 1\n",
    "                if pred != 0 and gold != 0 and pred == gold:\n",
    "                    cor += 1\n",
    "                if pred != 0:\n",
    "                    tot_pred += 1\n",
    "                   \n",
    "    acc = l_cor / l_tot\n",
    "    logger.info('Accuracy: %5f'%acc)\n",
    "    logger.info('Cor: %d, Pred TOT: %d, Gold TOT: %d'%(cor, tot_pred, tot_gold))\n",
    "    p = cor / tot_pred if cor > 0 else 0.0\n",
    "    r = cor / tot_gold if cor > 0 else 0.0\n",
    "    f1 = 2 * (p * r) / (p + r) if cor > 0 else 0.0\n",
    "    logger.info('P: %.5f, R: %.5f, F1: %.5f'%(p, r, f1))\n",
    "    logger.info('Used time: %f'%(time.time()-c_time))\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2124bf2-8ad4-4007-8d03-0b415ee4926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_ner_predictions(model, batches, dataset, output_file):\n",
    "    \"\"\"\n",
    "    Save the prediction as a json file\n",
    "    \"\"\"\n",
    "    ner_result = {}\n",
    "    span_hidden_table = {}\n",
    "    tot_pred_ett = 0\n",
    "    for i in range(len(batches)):\n",
    "        output_dict = model.run_batch(batches[i], training=False)\n",
    "        pred_ner = output_dict['pred_ner']\n",
    "        for sample, preds in zip(batches[i], pred_ner):\n",
    "            off = sample['sent_start_in_doc'] - sample['sent_start']\n",
    "            k = sample['doc_key'] + '-' + str(sample['sentence_ix'])\n",
    "            ner_result[k] = []\n",
    "            for span, pred in zip(sample['spans'], preds):\n",
    "                span_id = '%s::%d::(%d,%d)'%(sample['doc_key'], sample['sentence_ix'], span[0]+off, span[1]+off)\n",
    "                if pred == 0:\n",
    "                    continue\n",
    "                ner_result[k].append([span[0]+off, span[1]+off, ner_id2label[pred]])\n",
    "            tot_pred_ett += len(ner_result[k])\n",
    "\n",
    "    logger.info('Total pred entities: %d'%tot_pred_ett)\n",
    "\n",
    "    js = dataset.js\n",
    "    for i, doc in enumerate(js):\n",
    "        doc[\"predicted_ner\"] = []\n",
    "        doc[\"predicted_relations\"] = []\n",
    "        for j in range(len(doc[\"sentences\"])):\n",
    "            k = doc['doc_key'] + '-' + str(j)\n",
    "            if k in ner_result:\n",
    "                doc[\"predicted_ner\"].append(ner_result[k])\n",
    "            else:\n",
    "                logger.info('%s not in NER results!'%k)\n",
    "                doc[\"predicted_ner\"].append([])\n",
    "            \n",
    "            doc[\"predicted_relations\"].append([])\n",
    "\n",
    "        js[i] = doc\n",
    "\n",
    "    logger.info('Output predictions to %s..'%(output_file))\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(json.dumps(doc, cls=NpEncoder) for doc in js))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4e2f8486-6867-4fd8-a7ff-7250ec40f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ner_labels = {\n",
    "    'ace04': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n",
    "    'ace05': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n",
    "    'scierc': ['Method', 'OtherScientificTerm', 'Task', 'Generic', 'Material', 'Metric'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d67fc-28f7-4404-ad92-6635d00f67ff",
   "metadata": {},
   "source": [
    "### Run entity do eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "80b069ca-18d9-4075-87d3-1d0d13e0f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/scierc_data/processed_data/json'\n",
    "output_dir = os.getcwd() + '/scierc_models/ent-scib-ctx0/'\n",
    "task = 'scierc'\n",
    "max_span_length = 8\n",
    "context_window = 0\n",
    "eval_batch_size = 32\n",
    "test_pred_filename = 'ent_pred_test.json'\n",
    "dev_pred_filename = 'ent_pred_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f86cef2-d427-47d7-a3f1-1852cf1313d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2023 21:16:42 - INFO - root - # Overlap: 0\n",
      "11/15/2023 21:16:42 - INFO - root - Extracted 275 samples from 50 documents, with 811 NER labels, 23.713 avg input length, 68 max length\n",
      "11/15/2023 21:16:42 - INFO - root - Max Length: 68, max NER: 11\n",
      "11/15/2023 21:16:42 - INFO - root - Loading BERT model from C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - Model name 'C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - Didn't find file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//added_tokens.json. We won't load it.\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - Didn't find file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//tokenizer.json. We won't load it.\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - loading file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//vocab.txt\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - loading file None\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - loading file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//special_tokens_map.json\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - loading file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//tokenizer_config.json\n",
      "11/15/2023 21:16:42 - INFO - transformers.tokenization_utils_base - loading file None\n",
      "11/15/2023 21:16:42 - INFO - transformers.configuration_utils - loading configuration file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//config.json\n",
      "11/15/2023 21:16:42 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertNerRe\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "11/15/2023 21:16:42 - INFO - transformers.modeling_utils - loading weights file C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//pytorch_model.bin\n",
      "11/15/2023 21:16:44 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BertForEntity.\n",
      "\n",
      "11/15/2023 21:16:44 - INFO - transformers.modeling_utils - All the weights of BertForEntity were initialized from the model checkpoint at C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0//.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForEntity for predictions without further training.\n",
      "11/15/2023 21:16:44 - INFO - root - Moving to CUDA...\n",
      "11/15/2023 21:16:45 - INFO - root - # GPUs = 1\n",
      "11/15/2023 21:16:45 - INFO - root - # Overlap: 0\n",
      "11/15/2023 21:16:45 - INFO - root - Extracted 551 samples from 100 documents, with 1685 NER labels, 24.321 avg input length, 97 max length\n",
      "11/15/2023 21:16:45 - INFO - root - Max Length: 97, max NER: 13\n",
      "11/15/2023 21:16:45 - INFO - root - Evaluating...\n",
      "11/15/2023 21:16:59 - INFO - root - Accuracy: 0.990194\n",
      "11/15/2023 21:16:59 - INFO - root - Cor: 1122, Pred TOT: 1680, Gold TOT: 1685\n",
      "11/15/2023 21:16:59 - INFO - root - P: 0.66786, R: 0.66588, F1: 0.66686\n",
      "11/15/2023 21:16:59 - INFO - root - Used time: 13.669825\n",
      "11/15/2023 21:17:12 - INFO - root - Total pred entities: 1680\n",
      "11/15/2023 21:17:12 - INFO - root - Output predictions to C:\\Users\\odaim\\Documents\\PURE reproduction/scierc_models/ent-scib-ctx0/ent_pred_test.json..\n"
     ]
    }
   ],
   "source": [
    "train_data = os.path.join(data_dir, 'train.json')\n",
    "dev_data = os.path.join(data_dir, 'dev.json')\n",
    "test_data = os.path.join(data_dir, 'test.json')\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "ner_label2id, ner_id2label = get_labelmap(task_ner_labels[task])\n",
    "\n",
    "dev_data = Dataset(dev_data)\n",
    "dev_samples, dev_ner = convert_dataset_to_samples(dev_data, max_span_length, ner_label2id=ner_label2id, context_window=context_window)\n",
    "dev_batches = batchify(dev_samples, eval_batch_size)\n",
    "\n",
    "bert_model_dir = output_dir\n",
    "num_ner_labels = len(task_ner_labels[task]) + 1\n",
    "model = EntityModel(model='allenai/scibert_scivocab_uncased', bert_model_dir=bert_model_dir, use_albert=False, max_span_length=max_span_length, num_ner_labels=num_ner_labels)\n",
    "\n",
    "test_data = Dataset(test_data)\n",
    "prediction_file = os.path.join(output_dir, test_pred_filename)\n",
    "\n",
    "test_samples, test_ner = convert_dataset_to_samples(test_data, max_span_length, ner_label2id=ner_label2id, context_window=context_window)\n",
    "test_batches = batchify(test_samples, eval_batch_size)\n",
    "evaluate(model, test_batches, test_ner)\n",
    "output_ner_predictions(model, test_batches, test_data, output_file=prediction_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
