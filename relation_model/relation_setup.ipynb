{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7145ef96-18f7-4c97-ba26-a3d20df5fcda",
   "metadata": {},
   "source": [
    "# Running the relation model\n",
    "In this notebook we will run and evalute the relation model proposed in the research paper [A Frustratingly Easy Approach for Entity and Relation Extraction](https://arxiv.org/pdf/2010.12812.pdf).\n",
    "\n",
    "This is a reproduction based on the instructions left by the authors in their [GitHub repo](https://github.com/princeton-nlp/PURE)\n",
    "\n",
    "**Environment information**\n",
    "\n",
    "- Windows 11\n",
    "- Python 3.6.13\n",
    "- pip 21.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c0e8b-76a1-468e-b48a-9b473be65ca7",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "Firstly, we setup our notebook by importing needed libraries and modules.\n",
    "\n",
    "And we initialize a logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c9995f-df21-4e6c-bed8-d759200ef493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\odaim\\anaconda3\\envs\\PUREReprodcution\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers import AlbertModel, AlbertPreTrainedModel\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger('run_relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51decb14-3791-4e74-affc-f40cf2917b08",
   "metadata": {},
   "source": [
    "## Main classes\n",
    "The authors have implemented their system in an OOP style. Let's go through the defined classes and understand what they do.\n",
    "\n",
    "### BertForRelation\n",
    "This Python class is a custom implementation of a relation classification model based on the BERT (Bidirectional Encoder Representations from Transformers) architecture. Let me break down the key components:\n",
    "This class inherits from BertPreTrainedModel. Therefore, it's building on top of an existing BERT model. This is done to leverage th pre-trained BERT model's weights and then fine-tune it for our specific task.\n",
    "\n",
    "The BertForRelation class is designed for relation classification tasks, where the goal is to predict the relationship between entities in a given text. It leverages the BERT (Bidirectional Encoder Representations from Transformers) architecture and extends the BertPreTrainedModel class, building upon a pre-trained BERT model.\n",
    "\n",
    "The constructor initializes various components of the model:\n",
    "\n",
    "- config: BERT model configuration.\n",
    "- num_rel_labels: Number of distinct relation labels.\n",
    "\n",
    "The setup includes:\n",
    "\n",
    "- bert: The BERT model initialized with the provided configuration.\n",
    "- dropout: A dropout layer with a dropout probability specified in the BERT configuration.\n",
    "- layer_norm: Layer normalization applied to the concatenated output of the subject and object representations.\n",
    "- classifier: A linear layer for classification, mapping the concatenated representation to the output logits.\n",
    "- init_weights(): Initialization of the model's weights.\n",
    "\n",
    "We must note the setup of the model:\n",
    "\n",
    "```python\n",
    "self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "```\n",
    "\n",
    "The model is a linear layer that takes the concatenated representation of subject and object entities as input and produces logits for each possible relation label.\n",
    "\n",
    "\n",
    "- It's defined using nn.Linear and consists of two layers.\n",
    "- Input dimension (input_dim) is set to the sum of the following:\n",
    "  - config.hidden_size: corresponds to the hidden size of the BERT model. In BERT, each token in the input sequence is associated with a hidden vector of this size.\n",
    "  - * 2: The * 2 indicates that the representations of the subject and object entities are concatenated. So, the input dimension is twice the hidden size. This is likely because the model wants to capture information from both the start and end embeddings of the entities.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "`self.num_labels` is the number of distinct relation labels that the model is designed to classify. Each output neuron in the linear layer corresponds to a specific relation label.\n",
    "\n",
    "The overall architecture is a simple linear transformation that maps the concatenated representation of subject and object entities to a vector of logits, where each element of the vector corresponds to the model's prediction for a specific relation label. This linear layer is typically followed by a softmax activation during training to convert the logits into probabilities and compute the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a60c06-d042-4fa9-a2a0-8daba9d62f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForRelation(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_rel_labels):\n",
    "        super(BertForRelation, self).__init__(config)\n",
    "        self.num_labels = num_rel_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.layer_norm = BertLayerNorm(config.hidden_size * 2)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, sub_idx=None, obj_idx=None, input_position=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=False, output_attentions=False, position_ids=input_position)\n",
    "        sequence_output = outputs[0]\n",
    "        sub_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, sub_idx)])\n",
    "        obj_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, obj_idx)])\n",
    "        rep = torch.cat((sub_output, obj_output), dim=1)\n",
    "        rep = self.layer_norm(rep)\n",
    "        rep = self.dropout(rep)\n",
    "        logits = self.classifier(rep)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba491f0-3343-4ccf-9d62-820f4794881f",
   "metadata": {},
   "source": [
    "### AlbertForRelation\n",
    "\n",
    "There's also the class AlbertForEntity. Which is very similar to the BertForEntity with a key difference in the underlying transformer architecture they use: BertForRelation is based on BERT, while AlbertForRelation is based on ALBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33aafec3-d7f0-4a7c-9472-1a574176cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertForRelation(AlbertPreTrainedModel):\n",
    "    def __init__(self, config, num_rel_labels):\n",
    "        super(AlbertForRelation, self).__init__(config)\n",
    "        self.num_labels = num_rel_labels\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.layer_norm = BertLayerNorm(config.hidden_size * 2)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, sub_idx=None, obj_idx=None):\n",
    "        outputs = self.albert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=False, output_attentions=False)\n",
    "        sequence_output = outputs[0]\n",
    "        sub_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, sub_idx)])\n",
    "        obj_output = torch.cat([a[i].unsqueeze(0) for a, i in zip(sequence_output, obj_idx)])\n",
    "        rep = torch.cat((sub_output, obj_output), dim=1)\n",
    "        rep = self.layer_norm(rep)\n",
    "        rep = self.dropout(rep)\n",
    "        logits = self.classifier(rep)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081a11b-3e18-4310-bf9c-2cbb163685e1",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Represents a dataset. It's a convenient wrapper for handling datasets, reading data from JSON files, and creating Document objects.\n",
    "\n",
    "Let's break down its components:\n",
    "#### `__init__` method:\n",
    "\n",
    "- Initializes an instance of the Dataset class.\n",
    "- Takes three parameters: `json_file`, `pred_file` **(default is `None`)**, and `doc_range` **(default is `None`)**.\n",
    "- Reads data from the specified JSON files (`json_file` and `pred_file`).\n",
    "- If a document range (`doc_range`) is provided, it selects a subset of documents within that range.\n",
    "- Creates a list of Document objects based on the read data.\n",
    "\n",
    "#### `update_from_js` method:\n",
    "\n",
    "- Updates the dataset with new data (`js`).\n",
    "- Re-creates the list of `Document` objects based on the updated data.\n",
    "\n",
    "#### \\`_read method`:\n",
    "\n",
    "- Reads data from JSON files (`json_file` and optionally `pred_file`).\n",
    "If `pred_file` is provided, it merges the data from the gold (`gold_docs`) and predicted (`pred_docs`) files.\n",
    "- Returns the merged list of documents.\n",
    "\n",
    "#### `__getitem__` method:\n",
    "\n",
    "- Enables indexing of the dataset. Given an index `ix`, it returns the corresponding Document object.\n",
    "\n",
    "#### `__len__` method:\n",
    "\n",
    "- Returns the length of the dataset, i.e., the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73432310-8161-4745-b55a-f93e909e4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, json_file, pred_file=None, doc_range=None):\n",
    "        self.js = self._read(json_file, pred_file)\n",
    "        if doc_range is not None:\n",
    "            self.js = self.js[doc_range[0]:doc_range[1]]\n",
    "        self.documents = [Document(js) for js in self.js]\n",
    "\n",
    "    def update_from_js(self, js):\n",
    "        self.js = js\n",
    "        self.documents = [Document(js) for js in self.js]\n",
    "\n",
    "    def _read(self, json_file, pred_file=None):\n",
    "        gold_docs = [json.loads(line) for line in open(json_file)]\n",
    "        if pred_file is None:\n",
    "            return gold_docs\n",
    "\n",
    "        pred_docs = [json.loads(line) for line in open(pred_file)]\n",
    "        merged_docs = []\n",
    "        for gold, pred in zip(gold_docs, pred_docs):\n",
    "            assert gold[\"doc_key\"] == pred[\"doc_key\"]\n",
    "            assert gold[\"sentences\"] == pred[\"sentences\"]\n",
    "            merged = copy.deepcopy(gold)\n",
    "            for k, v in pred.items():\n",
    "                if \"predicted\" in k:\n",
    "                    merged[k] = v\n",
    "            merged_docs.append(merged)\n",
    "\n",
    "        return merged_docs\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.documents[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f5272-c06f-454b-a090-6598218876b0",
   "metadata": {},
   "source": [
    "### Document\n",
    "\n",
    "This class represents a document. It encapsulates information about a document, its sentences, and any associated clusters. It provides methods for accessing and manipulating this information.\n",
    "\n",
    "Let's go through each part:\n",
    "\n",
    "#### `__init__` method:\n",
    "\n",
    "- Initializes an instance of the Document class.\n",
    "- Takes a JSON object (`js`) as input.\n",
    "- Extracts the document key (`_doc_key`) from the JSON object.\n",
    "- Uses the `fields_to_batches` function to extract specific fields from the JSON and create a list of entries.\n",
    "- Computes sentence lengths and starts to facilitate sentence indexing.\n",
    "- Creates a list of Sentence objects based on the entries.\n",
    "- If \"`clusters`\" or \"`predicted_clusters`\" are present in the JSON, creates lists of Cluster objects for clusters and predicted clusters.\n",
    "\n",
    "#### `__repr__` method:\n",
    "\n",
    "- Returns a string representation of the document, including sentence indices and their corresponding text.\n",
    "\n",
    "#### `__getitem__` method:\n",
    "\n",
    "- Enables indexing of the document. Given an index ix, it returns the corresponding `Sentence` object.\n",
    "\n",
    "#### `__len__` method:\n",
    "\n",
    "- Returns the number of sentences in the document.\n",
    "\n",
    "#### `print_plaintext method`:\n",
    "\n",
    "- Prints the plaintext representation of the document, sentence by sentence.\n",
    "\n",
    "#### `find_cluster` method:\n",
    "\n",
    "- Searches through reference clusters (either predicted or actual) to find the one containing a specified entity.\n",
    "- Returns the found cluster or None if no match is found.\n",
    "\n",
    "#### `n_tokens property`:\n",
    "\n",
    "- Returns the total number of tokens in the document by summing the number of tokens in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3ebedd-f439-4d22-a812-560ca07efe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, js):\n",
    "        self._doc_key = js[\"doc_key\"]\n",
    "        entries = fields_to_batches(js, [\"doc_key\", \"clusters\", \"predicted_clusters\", \"section_starts\"])\n",
    "        sentence_lengths = [len(entry[\"sentences\"]) for entry in entries]\n",
    "        sentence_starts = np.cumsum(sentence_lengths)\n",
    "        sentence_starts = np.roll(sentence_starts, 1)\n",
    "        sentence_starts[0] = 0\n",
    "        self.sentence_starts = sentence_starts\n",
    "        self.sentences = [Sentence(entry, sentence_start, sentence_ix)\n",
    "                          for sentence_ix, (entry, sentence_start)\n",
    "                          in enumerate(zip(entries, sentence_starts))]\n",
    "        if \"clusters\" in js:\n",
    "            self.clusters = [Cluster(entry, i, self)\n",
    "                             for i, entry in enumerate(js[\"clusters\"])]\n",
    "        if \"predicted_clusters\" in js:\n",
    "            self.predicted_clusters = [Cluster(entry, i, self)\n",
    "                                       for i, entry in enumerate(js[\"predicted_clusters\"])]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([str(i) + \": \" + \" \".join(sent.text) for i, sent in enumerate(self.sentences)])\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def print_plaintext(self):\n",
    "        for sent in self:\n",
    "            print(\" \".join(sent.text))\n",
    "\n",
    "\n",
    "    def find_cluster(self, entity, predicted=True):\n",
    "        \"\"\"\n",
    "        Search through erence clusters and return the one containing the query entity, if it's\n",
    "        part of a cluster. If we don't find a match, return None.\n",
    "        \"\"\"\n",
    "        clusters = self.predicted_clusters if predicted else self.clusters\n",
    "        for clust in clusters:\n",
    "            for entry in clust:\n",
    "                if entry.span == entity.span:\n",
    "                    return clust\n",
    "\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return sum([len(sent) for sent in self.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e81cad-9a62-40fb-b427-6958a22f4759",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "The `Cluster` class represents a cluster of entities within a document. It is used to group together entities that belong to the same cluster, providing information about the members of the cluster.\n",
    "This class is designed to encapsulate information about a cluster of entities within a document. It facilitates the organization and representation of entities belonging to the same cluster.\n",
    "\n",
    "#### Initialization\n",
    "- Parameters:\n",
    "  - `cluster`: A list of entries representing the entities in the cluster.\n",
    "  - `cluster_id`: An identifier for the cluster.\n",
    "  - `document`: The document object to which the cluster belongs.\n",
    "- Attributes\n",
    "  - `members`: A list of ClusterMember instances, each representing an entity in the cluster.\n",
    "  - `cluster_id`: The identifier for the cluster.\n",
    "  \n",
    "#### Initialization Details\n",
    "- The `__init__` method initializes the cluster by extracting information about each entity in the cluster.\n",
    "- For each entry in the cluster, it determines the corresponding sentence and span in the document.\n",
    "- It creates `ClusterMember` instances for each entity and appends them to the members list.\n",
    "\n",
    "#### Representation\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the cluster, including the cluster identifier and a representation of its members.\n",
    "Accessing Members\n",
    "\n",
    "`__getitem__` Method:\n",
    "Allows accessing individual members of the cluster using indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf27b5db-6ece-4a9e-9e13-24e60c57ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, cluster, cluster_id, document):\n",
    "        members = []\n",
    "        for entry in cluster:\n",
    "            sentence_ix = get_sentence_of_span(entry, document.sentence_starts, document.n_tokens)\n",
    "            sentence = document[sentence_ix]\n",
    "            span = Span(entry[0], entry[1], sentence.text, sentence.sentence_start)\n",
    "            ners = [x for x in sentence.ner if x.span == span]\n",
    "            assert len(ners) <= 1\n",
    "            ner = ners[0] if len(ners) == 1 else None\n",
    "            to_append = ClusterMember(span, ner, sentence, cluster_id)\n",
    "            members.append(to_append)\n",
    "\n",
    "        self.members = members\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.cluster_id}: \" + self.members.__repr__()\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.members[ix]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784f412-5c4f-486c-bcd8-1b3294e894af",
   "metadata": {},
   "source": [
    "### ClusterMember\n",
    "\n",
    "Represents an individual entity within a cluster. It provides information about the entity's span, associated named entity recognition (NER) information, the sentence it belongs to, and the identifier of the cluster to which it is assigned. The class serves as a container for information about an individual entity within a cluster. It encapsulates details such as the span, NER information, sentence context, and the cluster to which the entity is assigned.\n",
    "\n",
    "#### Initialization\n",
    "- Parameters:\n",
    "  - `span`: A Span instance representing the span of the entity in the document.\n",
    "  - `ner`: A NER instance representing the NER information for the entity.\n",
    "  - `sentence`: A Sentence instance representing the sentence containing the entity.\n",
    "  - `cluster_id`: The identifier of the cluster to which the entity belongs.\n",
    "- Attributes\n",
    "  - `span`: A Span instance representing the span of the entity.\n",
    "  - `ner`: A NER instance representing the NER information for the entity.\n",
    "  - `sentence`: A Sentence instance representing the sentence containing the entity.\n",
    "  - `cluster_id`: The identifier of the cluster to which the entity belongs.\n",
    "  \n",
    "#### Initialization Details\n",
    "\n",
    "The `__init__` method initializes a `ClusterMember` by assigning values to its attributes based on the provided parameters.\n",
    "Representation\n",
    "\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the `ClusterMember`, including the sentence index and a representation of its span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97932254-c9ae-4c58-82f4-200ea937605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterMember:\n",
    "    def __init__(self, span, ner, sentence, cluster_id):\n",
    "        self.span = span\n",
    "        self.ner = ner\n",
    "        self.sentence = sentence\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.sentence.sentence_ix}> \" + self.span.__repr__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8bb7fc-7a06-4058-9b64-0c87a0313500",
   "metadata": {},
   "source": [
    "### Sentence\n",
    "\n",
    "This class represents a sentence. It encapsulates information about a sentence, including its text and associated entities (NER, relations, events). It provides methods for accessing and manipulating this information.\n",
    "\n",
    "Let's go through each method:\n",
    "\n",
    "#### `__init__` method:\n",
    "\n",
    "- Initializes an instance of the `Sentence` class.\n",
    "- Takes an `entry`, `sentence_start` (index of the sentence start in the document), and `sentence_ix` (sentence index) as input.\n",
    "- Stores information about the sentence's start position, text, and index.\n",
    "- Parses gold entities (NER, relations, events) and predicted entities (NER, relations, events).\n",
    "- Stores top spans if available.\n",
    "\n",
    "#### `__repr__` method:\n",
    "\n",
    "- Returns a string representation of the sentence, including the text and token indices.\n",
    "\n",
    "#### `__len__` method:\n",
    "\n",
    "-Returns the number of tokens in the sentence.\n",
    "\n",
    "#### `get_flavor` method:\n",
    "\n",
    "- Given an argument (presumably an entity), retrieves its flavor from the gold NER annotations.\n",
    "- If multiple NER annotations are found for the same span, prints a message (debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3030363c-9fab-41ec-a724-e5a7b404d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, entry, sentence_start, sentence_ix):\n",
    "        self.sentence_start = sentence_start\n",
    "        self.text = entry[\"sentences\"]\n",
    "        self.sentence_ix = sentence_ix\n",
    "        # Gold\n",
    "        if \"ner_flavor\" in entry:\n",
    "            self.ner = [NER(this_ner, self.text, sentence_start, flavor=this_flavor)\n",
    "                        for this_ner, this_flavor in zip(entry[\"ner\"], entry[\"ner_flavor\"])]\n",
    "        elif \"ner\" in entry:\n",
    "            self.ner = [NER(this_ner, self.text, sentence_start)\n",
    "                        for this_ner in entry[\"ner\"]]\n",
    "        if \"relations\" in entry:\n",
    "            self.relations = [Relation(this_relation, self.text, sentence_start) for\n",
    "                              this_relation in entry[\"relations\"]]\n",
    "        if \"events\" in entry:\n",
    "            self.events = Events(entry[\"events\"], self.text, sentence_start)\n",
    "\n",
    "        # Predicted\n",
    "        if \"predicted_ner\" in entry:\n",
    "            self.predicted_ner = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
    "                                  this_ner in entry[\"predicted_ner\"]]\n",
    "        if \"predicted_relations\" in entry:\n",
    "            self.predicted_relations = [Relation(this_relation, self.text, sentence_start) for\n",
    "                                        this_relation in entry[\"predicted_relations\"]]\n",
    "        if \"predicted_events\" in entry:\n",
    "            self.predicted_events = Events(entry[\"predicted_events\"], self.text, sentence_start)\n",
    "\n",
    "        # Top spans\n",
    "        if \"top_spans\" in entry:\n",
    "            self.top_spans = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
    "                                this_ner in entry[\"top_spans\"]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        the_text = \" \".join(self.text)\n",
    "        the_lengths = np.array([len(x) for x in self.text])\n",
    "        tok_ixs = \"\"\n",
    "        for i, offset in enumerate(the_lengths):\n",
    "            true_offset = offset if i < 10 else offset - 1\n",
    "            tok_ixs += str(i)\n",
    "            tok_ixs += \" \" * true_offset\n",
    "\n",
    "        return the_text + \"\\n\" + tok_ixs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def get_flavor(self, argument):\n",
    "        the_ner = [x for x in self.ner if x.span == argument.span]\n",
    "        if len(the_ner) > 1:\n",
    "            print(\"Weird\")\n",
    "        if the_ner:\n",
    "            the_flavor = the_ner[0].flavor\n",
    "        else:\n",
    "            the_flavor = None\n",
    "        return the_flavor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a8171-da41-4415-8477-628cc48c8941",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "The NER class represents a Named Entity Recognition annotation within a sentence. It encapsulates information about a named entity, providing methods for representation and equality checks. The `Span` (we'll get to it next) class is used to represent the span of the named entity within the context of the sentence.\n",
    "\n",
    "Here are some details about it:\n",
    "#### Initialization\n",
    "\n",
    "- Parameters:\n",
    "  - `ner`: A list containing information about the NER span, including start index, end index, and label.\n",
    "  - `text`: The text content of the sentence.\n",
    "  - `sentence_start`: The index of the sentence start in the document.\n",
    "  - `flavor`: An optional parameter representing the flavor or type of the named entity.\n",
    "  \n",
    "- Attributes:\n",
    "  - `span`: An instance of the Span class representing the span of the NER annotation.\n",
    "  - `label`: The label assigned to the NER entity.\n",
    "  - `flavor`: The flavor or type of the named entity.\n",
    "  \n",
    "- Representation\n",
    "\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the NER annotation, including the span and label.\n",
    "Equality Check\n",
    "\n",
    "`__eq__` Method:\n",
    "Checks if two NER instances are equal by comparing their span, label, and flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d638650-dbb5-4289-99b5-52ff47bff05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER:\n",
    "    def __init__(self, ner, text, sentence_start, flavor=None):\n",
    "        self.span = Span(ner[0], ner[1], text, sentence_start)\n",
    "        self.label = ner[2]\n",
    "        self.flavor = flavor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.span.__repr__() + \": \" + self.label\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.span == other.span and\n",
    "                self.label == other.label and\n",
    "                self.flavor == other.flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3757870-6e1c-4a41-9f6e-84ce7bcb6aff",
   "metadata": {},
   "source": [
    "### Span\n",
    "\n",
    "The `Span` class represents a span of text within a document. It encapsulate information about a text span, providing methods for representation, equality checks, and hashing. It ensures the proper handling of spans within the document and sentence contexts.\n",
    "\n",
    "#### Initialization\n",
    "- Parameters:\n",
    "  - `start`: The starting index of the span in the entire document.\n",
    "  - `end`: The ending index of the span in the entire document.\n",
    "  - `text`: The text content of the entire document.\n",
    "  - `sentence_start`: The index of the sentence start in the document.\n",
    "  \n",
    "- Attributes\n",
    "  - `start_doc`, `end_doc`: The start and end indices of the span in the entire document.\n",
    "  - `span_doc`: A tuple representing the span in the entire document.\n",
    "  - `start_sent`, `end_sent`: The start and end indices of the span within the sentence.\n",
    "  - `span_sent`: A tuple representing the span within the sentence.\n",
    "  - `text`: The actual text content of the span.\n",
    "  \n",
    "#### Representation\n",
    "\n",
    "`__repr__` Method:\n",
    "Returns a string representation of the span, including start and end indices and the actual text content.\n",
    "Equality Check\n",
    "\n",
    "`__eq__` Method:\n",
    "Checks if two Span instances are equal by comparing their spans in both the document and the sentence, along with the text content.\n",
    "Hashing\n",
    "\n",
    "`__hash__` Method:\n",
    "Computes a hash value for the Span instance based on its document and sentence spans, as well as the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbea2e72-2d85-44e1-9de5-fa9f7d4d5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, start, end, text, sentence_start):\n",
    "        self.start_doc = start\n",
    "        self.end_doc = end\n",
    "        self.span_doc = (self.start_doc, self.end_doc)\n",
    "        self.start_sent = start - sentence_start\n",
    "        self.end_sent = end - sentence_start\n",
    "        self.span_sent = (self.start_sent, self.end_sent)\n",
    "        self.text = text[self.start_sent:self.end_sent + 1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str((self.start_sent, self.end_sent, self.text))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.span_doc == other.span_doc and\n",
    "                self.span_sent == other.span_sent and\n",
    "                self.text == other.text)\n",
    "\n",
    "    def __hash__(self):\n",
    "        tup = self.span_doc + self.span_sent + (\" \".join(self.text),)\n",
    "        return hash(tup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6135ff-3aab-4f09-8292-7a4091f8466c",
   "metadata": {},
   "source": [
    "### Relation\n",
    "\n",
    "The Relation class is designed to represent a relation between two spans within a text. It encapsulates information about the spans, such as their start and end positions, the text they cover, and the label assigned to the relation. Here's a breakdown of the class:\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "  - `relation`: A tuple representing the start and end positions of the two spans and the label of the relation.\n",
    "  - `text`: The text containing the spans.\n",
    "  - `sentence_start`: The starting position of the sentence in the text.\n",
    "  \n",
    "- Attributes\n",
    "\n",
    "  - `pair`: A tuple containing two Span objects (span1 and span2) representing the spans of the relation.\n",
    "  - `label`: The label of the relation.\n",
    "  \n",
    "#### Representation\n",
    "`__repr__` Method: Returns a string representation of the Relation object, including the string representations of the two spans, the relation between them (\", \"), and the label.\n",
    "\n",
    "`__eq__` Method: Checks if two Relation objects are equal by comparing their span pairs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c87a147-2a87-4b2b-a7de-d0f14b5b84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relation:\n",
    "    def __init__(self, relation, text, sentence_start):\n",
    "        start1, end1 = relation[0], relation[1]\n",
    "        start2, end2 = relation[2], relation[3]\n",
    "        label = relation[4]\n",
    "        span1 = Span(start1, end1, text, sentence_start)\n",
    "        span2 = Span(start2, end2, text, sentence_start)\n",
    "        self.pair = (span1, span2)\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.pair[0].__repr__() + \", \" + self.pair[1].__repr__() + \": \" + self.label\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.pair == other.pair) and (self.label == other.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74418c55-1311-401e-ab9a-5fc7e700b0d5",
   "metadata": {},
   "source": [
    "### InputFeatures\n",
    "\n",
    "This class represents a single set of features for the relation model. It encapsulates the necessary information required for processing a single example during training or inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ecb42b4-7cfb-4739-b491-cd2f16529d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, sub_idx, obj_idx):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.sub_idx = sub_idx\n",
    "        self.obj_idx = obj_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898aefe7-a29f-46dc-bf24-b5c5b5ffd80c",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "The authors have aslo implemented some utility functions. Let's go through them and understand what they do.\n",
    "\n",
    "### add_marker_tokens\n",
    "This function, as discussed by the authors, extends the tokenizer's vocabulary with special marker tokens.\n",
    "\n",
    "Therfore, providing additional information about the structure and types of entities in the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c71fcf9-94fa-4453-b766-0052e7fc38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_marker_tokens(tokenizer, ner_labels):\n",
    "    new_tokens = ['<SUBJ_START>', '<SUBJ_END>', '<OBJ_START>', '<OBJ_END>']\n",
    "    for label in ner_labels:\n",
    "        new_tokens.append('<SUBJ_START=%s>'%label)\n",
    "        new_tokens.append('<SUBJ_END=%s>'%label)\n",
    "        new_tokens.append('<OBJ_START=%s>'%label)\n",
    "        new_tokens.append('<OBJ_END=%s>'%label)\n",
    "    for label in ner_labels:\n",
    "        new_tokens.append('<SUBJ=%s>'%label)\n",
    "        new_tokens.append('<OBJ=%s>'%label)\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "    logger.info('# vocab after adding markers: %d'%len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9af336-8c48-4545-a914-57f4e220fb36",
   "metadata": {},
   "source": [
    "### convert_examples_to_features\n",
    "\n",
    "The convert_examples_to_features function is responsible for converting the list of input examples into a list of InputFeatures objects, which can be used as input to the relation classification. It tokenizes the input examples, adds special markers for subject and object entities, handles token length constraints, and prepares the data for model input.\n",
    "\n",
    "**Let's break down the key components of this function:**\n",
    "\n",
    "#### Input Parameters:\n",
    "- `examples`: A list of input examples, where each example is a dictionary containing information about tokens, subject and object entities, and the relation.\n",
    "- `label2id`: A dictionary mapping relation labels to their corresponding integer IDs.\n",
    "- `max_seq_length`: The maximum sequence length allowed for the input tokens.\n",
    "- `tokenizer`: The tokenizer used to tokenize input sequences.\n",
    "- `special_tokens`: A dictionary used to keep track of special tokens introduced during processing.\n",
    "- `unused_tokens`: A boolean indicating whether tokens should be used as special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cdae56d-073e-4ca4-ba7f-4f71ff447acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label2id, max_seq_length, tokenizer, special_tokens, unused_tokens=True):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of `InputBatch`s.\n",
    "    unused_tokens: whether use [unused1] [unused2] as special tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def get_special_token(w):\n",
    "        if w not in special_tokens:\n",
    "            if unused_tokens:\n",
    "                special_tokens[w] = \"[unused%d]\" % (len(special_tokens) + 1)\n",
    "            else:\n",
    "                special_tokens[w] = ('<' + w + '>').lower()\n",
    "        return special_tokens[w]\n",
    "\n",
    "    num_tokens = 0\n",
    "    max_tokens = 0\n",
    "    num_fit_examples = 0\n",
    "    num_shown_examples = 0\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens = [CLS]\n",
    "        SUBJECT_START = get_special_token(\"SUBJ_START\")\n",
    "        SUBJECT_END = get_special_token(\"SUBJ_END\")\n",
    "        OBJECT_START = get_special_token(\"OBJ_START\")\n",
    "        OBJECT_END = get_special_token(\"OBJ_END\")\n",
    "        SUBJECT_NER = get_special_token(\"SUBJ=%s\" % example['subj_type'])\n",
    "        OBJECT_NER = get_special_token(\"OBJ=%s\" % example['obj_type'])\n",
    "\n",
    "        SUBJECT_START_NER = get_special_token(\"SUBJ_START=%s\"%example['subj_type'])\n",
    "        SUBJECT_END_NER = get_special_token(\"SUBJ_END=%s\"%example['subj_type'])\n",
    "        OBJECT_START_NER = get_special_token(\"OBJ_START=%s\"%example['obj_type'])\n",
    "        OBJECT_END_NER = get_special_token(\"OBJ_END=%s\"%example['obj_type'])\n",
    "\n",
    "        for i, token in enumerate(example['token']):\n",
    "            if i == example['subj_start']:\n",
    "                sub_idx = len(tokens)\n",
    "                tokens.append(SUBJECT_START_NER)\n",
    "            if i == example['obj_start']:\n",
    "                obj_idx = len(tokens)\n",
    "                tokens.append(OBJECT_START_NER)\n",
    "            for sub_token in tokenizer.tokenize(token):\n",
    "                tokens.append(sub_token)\n",
    "            if i == example['subj_end']:\n",
    "                tokens.append(SUBJECT_END_NER)\n",
    "            if i == example['obj_end']:\n",
    "                tokens.append(OBJECT_END_NER)\n",
    "        tokens.append(SEP)\n",
    "\n",
    "        num_tokens += len(tokens)\n",
    "        max_tokens = max(max_tokens, len(tokens))\n",
    "\n",
    "        if len(tokens) > max_seq_length:\n",
    "            tokens = tokens[:max_seq_length]\n",
    "            if sub_idx >= max_seq_length:\n",
    "                sub_idx = 0\n",
    "            if obj_idx >= max_seq_length:\n",
    "                obj_idx = 0\n",
    "        else:\n",
    "            num_fit_examples += 1\n",
    "\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        label_id = label2id[example['relation']]\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if num_shown_examples < 20:\n",
    "            if (ex_index < 5) or (label_id > 0):\n",
    "                num_shown_examples += 1\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"guid: %s\" % (example['id']))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(\n",
    "                        [str(x) for x in tokens]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                logger.info(\"label: %s (id = %d)\" % (example['relation'], label_id))\n",
    "                logger.info(\"sub_idx, obj_idx: %d, %d\" % (sub_idx, obj_idx))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id,\n",
    "                              sub_idx=sub_idx,\n",
    "                              obj_idx=obj_idx))\n",
    "    logger.info(\"Average #tokens: %.2f\" % (num_tokens * 1.0 / len(examples)))\n",
    "    logger.info(\"Max #tokens: %d\"%max_tokens)\n",
    "    logger.info(\"%d (%.2f %%) examples can fit max_seq_length = %d\" % (num_fit_examples,\n",
    "                num_fit_examples * 100.0 / len(examples), max_seq_length))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d47f0-8e66-434f-b4df-7e4819bc6f2c",
   "metadata": {},
   "source": [
    "### setseed\n",
    "The setseed function sets the random seed for various random number generators in the environment, specifically for the `random` module, `numpy` library, and the `PyTorch` library.\n",
    "\n",
    "Setting a random seed ensures reproducibility in the generation of random numbers, which is crucial for obtaining consistent results when running experiments or training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c0c604f-380e-4f41-a5b0-62b59ba851b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setseed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a1ca7-51f1-470f-8eba-488ece0fbe59",
   "metadata": {},
   "source": [
    "### simple_accuracy\n",
    "This function calculates the accuracy of a model's predictions by comparing the predicted labels (`preds`) with the actual ground truth labels (`labels`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f309e0c-02b8-47ec-a482-62cb017c9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e76ab2-b1de-47d8-a63c-040e03d30d3f",
   "metadata": {},
   "source": [
    "### compute_f1\n",
    "\n",
    "This function is used to comute the F1 score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e72b43a-4325-4eb3-90ce-091701eb3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(preds, labels, e2e_ngold):\n",
    "    n_gold = n_pred = n_correct = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        if pred != 0:\n",
    "            n_pred += 1\n",
    "        if label != 0:\n",
    "            n_gold += 1\n",
    "        if (pred != 0) and (label != 0) and (pred == label):\n",
    "            n_correct += 1\n",
    "    if n_correct == 0:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    else:\n",
    "        prec = n_correct * 1.0 / n_pred\n",
    "        recall = n_correct * 1.0 / n_gold\n",
    "        if prec + recall > 0:\n",
    "            f1 = 2.0 * prec * recall / (prec + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "        if e2e_ngold is not None:\n",
    "            e2e_recall = n_correct * 1.0 / e2e_ngold\n",
    "            e2e_f1 = 2.0 * prec * e2e_recall / (prec + e2e_recall)\n",
    "        else:\n",
    "            e2e_recall = e2e_f1 = 0.0\n",
    "        return {'precision': prec, 'recall': e2e_recall, 'f1': e2e_f1, 'task_recall': recall, 'task_f1': f1, \n",
    "        'n_correct': n_correct, 'n_pred': n_pred, 'n_gold': e2e_ngold, 'task_ngold': n_gold}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5c8c2-e94c-4402-b2bc-17cc9d21d5e8",
   "metadata": {},
   "source": [
    "### evaluate\n",
    "\n",
    "Evaluates a trained model on an evaluation dataset. It computes various evaluation metrics, including loss, F1 score, accuracy, and additional metrics provided by the compute_f1 function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1953e6e9-48cb-49e1-8494-77db5028f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, eval_dataloader, eval_label_ids, num_labels, e2e_ngold=None, verbose=True):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "    for input_ids, input_mask, segment_ids, label_ids, sub_idx, obj_idx in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        sub_idx = sub_idx.to(device)\n",
    "        obj_idx = obj_idx.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None, sub_idx=sub_idx, obj_idx=obj_idx)\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    logits = preds[0]\n",
    "    preds = np.argmax(preds[0], axis=1)\n",
    "    result = compute_f1(preds, eval_label_ids.numpy(), e2e_ngold=e2e_ngold)\n",
    "    result['accuracy'] = simple_accuracy(preds, eval_label_ids.numpy())\n",
    "    result['eval_loss'] = eval_loss\n",
    "    if verbose:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    return preds, result, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d65242-79df-4804-b930-960c0540aa5b",
   "metadata": {},
   "source": [
    "### print_pred_json\n",
    "\n",
    "This function is used for formatting and printing the predicted relations based on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39af037c-b43b-4006-aa64-29d4e0059140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_json(eval_data, eval_examples, preds, id2label, output_file):\n",
    "    rels = dict()\n",
    "    for ex, pred in zip(eval_examples, preds):\n",
    "        doc_sent, sub, obj = decode_sample_id(ex['id'])\n",
    "        if doc_sent not in rels:\n",
    "            rels[doc_sent] = []\n",
    "        if pred != 0:\n",
    "            rels[doc_sent].append([sub[0], sub[1], obj[0], obj[1], id2label[pred]])\n",
    "\n",
    "    js = eval_data.js\n",
    "    for doc in js:\n",
    "        doc['predicted_relations'] = []\n",
    "        for sid in range(len(doc['sentences'])):\n",
    "            k = '%s@%d'%(doc['doc_key'], sid)\n",
    "            doc['predicted_relations'].append(rels.get(k, []))\n",
    "    \n",
    "    logger.info('Output predictions to %s..'%(output_file))\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(json.dumps(doc) for doc in js))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb50b26-5267-4dc8-8dc0-1eb3bc19693a",
   "metadata": {},
   "source": [
    "### save_trained_model\n",
    "\n",
    "The save_trained_model function is responsible for saving the trained model, its configuration, and the associated tokenizer to the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bba02b22-fe74-438e-86f1-5f9fa3c02f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(output_dir, model, tokenizer):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    logger.info('Saving model to %s'%output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    model_to_save.config.to_json_file(output_config_file)\n",
    "    tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010091f-2cdb-4b28-8f75-5eba00986786",
   "metadata": {},
   "source": [
    "### generate_relation_data\n",
    "The generate_relation_data function is designed to prepare data for the training or evaluation of the relation model. It takes an entity dataset (`entity_data`), generates samples for relation extraction, and returns the dataset, a list of samples, and the total number of relations in the dataset. Let's break down the key components of this function:\n",
    "\n",
    "#### Input Parameters:\n",
    "- `entity_data`: The entity dataset from which relation data will be generated.\n",
    "- `use_gold`: A boolean parameter indicating whether to use gold (true) or predicted (false) named entities and relations. Default is False.\n",
    "- `context_window`: An integer representing the size of the context window to consider around each sentence. Default is 0.\n",
    "\n",
    "This function processes the entity dataset, extracting named entities and relations. It then generates samples for relation extraction, considering optional context windows. The generated samples include information about document ID, sentence ID, relation labels, entity spans, and tokenized text. The function returns the processed dataset, the list of generated samples, and the total number of relations in the dataset. The generated samples can be used for training or evaluating a relation extraction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88c5f983-c388-42e4-97b7-0504ef40a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation_data(entity_data, use_gold=False, context_window=0):\n",
    "    \"\"\"\n",
    "    Prepare data for the relation model\n",
    "    If training: set use_gold = True\n",
    "    \"\"\"\n",
    "    logger.info('Generate relation data from %s'%(entity_data))\n",
    "    data = Dataset(entity_data)\n",
    "\n",
    "    nner, nrel = 0, 0\n",
    "    max_sentsample = 0\n",
    "    samples = []\n",
    "    for doc in data:\n",
    "        for i, sent in enumerate(doc):\n",
    "            sent_samples = []\n",
    "\n",
    "            nner += len(sent.ner)\n",
    "            nrel += len(sent.relations)\n",
    "            if use_gold:\n",
    "                sent_ner = sent.ner\n",
    "            else:\n",
    "                sent_ner = sent.predicted_ner\n",
    "            \n",
    "            gold_ner = {}\n",
    "            for ner in sent.ner:\n",
    "                gold_ner[ner.span] = ner.label\n",
    "            \n",
    "            gold_rel = {}\n",
    "            for rel in sent.relations:\n",
    "                gold_rel[rel.pair] = rel.label\n",
    "            \n",
    "            sent_start = 0\n",
    "            sent_end = len(sent.text)\n",
    "            tokens = sent.text\n",
    "\n",
    "            if context_window > 0:\n",
    "                add_left = (context_window-len(sent.text)) // 2\n",
    "                add_right = (context_window-len(sent.text)) - add_left\n",
    "\n",
    "                j = i - 1\n",
    "                while j >= 0 and add_left > 0:\n",
    "                    context_to_add = doc[j].text[-add_left:]\n",
    "                    tokens = context_to_add + tokens\n",
    "                    add_left -= len(context_to_add)\n",
    "                    sent_start += len(context_to_add)\n",
    "                    sent_end += len(context_to_add)\n",
    "                    j -= 1\n",
    "\n",
    "                j = i + 1\n",
    "                while j < len(doc) and add_right > 0:\n",
    "                    context_to_add = doc[j].text[:add_right]\n",
    "                    tokens = tokens + context_to_add\n",
    "                    add_right -= len(context_to_add)\n",
    "                    j += 1\n",
    "            \n",
    "            for x in range(len(sent_ner)):\n",
    "                for y in range(len(sent_ner)):\n",
    "                    if x == y:\n",
    "                        continue\n",
    "                    sub = sent_ner[x]\n",
    "                    obj = sent_ner[y]\n",
    "                    label = gold_rel.get((sub.span, obj.span), 'no_relation')\n",
    "                    sample = {}\n",
    "                    sample['docid'] = doc._doc_key\n",
    "                    sample['id'] = '%s@%d::(%d,%d)-(%d,%d)'%(doc._doc_key, sent.sentence_ix, sub.span.start_doc, sub.span.end_doc, obj.span.start_doc, obj.span.end_doc)\n",
    "                    sample['relation'] = label\n",
    "                    sample['subj_start'] = sub.span.start_sent + sent_start\n",
    "                    sample['subj_end'] = sub.span.end_sent + sent_start\n",
    "                    sample['subj_type'] = sub.label\n",
    "                    sample['obj_start'] = obj.span.start_sent + sent_start\n",
    "                    sample['obj_end'] = obj.span.end_sent + sent_start\n",
    "                    sample['obj_type'] = obj.label\n",
    "                    sample['token'] = tokens\n",
    "                    sample['sent_start'] = sent_start\n",
    "                    sample['sent_end'] = sent_end\n",
    "\n",
    "                    sent_samples.append(sample)\n",
    "\n",
    "            max_sentsample = max(max_sentsample, len(sent_samples))\n",
    "            samples += sent_samples\n",
    "    \n",
    "    tot = len(samples)\n",
    "    logger.info('#samples: %d, max #sent.samples: %d'%(tot, max_sentsample))\n",
    "\n",
    "    return data, samples, nrel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5149aaa-7830-44bb-a386-386340518e5e",
   "metadata": {},
   "source": [
    "### decode_sample_id\n",
    "\n",
    "Used to decode a sample identified by its ID into: document and sentence information (`doc_sent`), subject coordinates (`sub`), and object coordinates (`obj`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98235e9b-c1d1-4dca-92b3-e146112e69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sample_id(sample_id):\n",
    "    doc_sent = sample_id.split('::')[0]\n",
    "    pair = sample_id.split('::')[1]\n",
    "    pair = pair.split('-')\n",
    "    sub = (int(pair[0][1:-1].split(',')[0]), int(pair[0][1:-1].split(',')[1]))\n",
    "    obj = (int(pair[1][1:-1].split(',')[0]), int(pair[1][1:-1].split(',')[1]))\n",
    "\n",
    "    return doc_sent, sub, obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98984a23-2b18-43ee-82ab-b5ab0df64fef",
   "metadata": {},
   "source": [
    "### get_sentence_of_span\n",
    "\n",
    "determine the index of the sentence to which a given span (represented as a pair of indices) belongs, and returns that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f0452b4-e30b-4b14-986c-1ba8cc0f4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_of_span(span, sentence_starts, doc_tokens):\n",
    "    \"\"\"\n",
    "    Return the index of the sentence that the span is part of.\n",
    "    \"\"\"\n",
    "    # Inclusive sentence ends\n",
    "    sentence_ends = [x - 1 for x in sentence_starts[1:]] + [doc_tokens - 1]\n",
    "    in_between = [span[0] >= start and span[1] <= end\n",
    "                  for start, end in zip(sentence_starts, sentence_ends)]\n",
    "    assert sum(in_between) == 1\n",
    "    the_sentence = in_between.index(True)\n",
    "    return the_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a955b1-2bcb-45d0-96ee-9389635d7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fields_to_batches(d, keys_to_ignore=[]):\n",
    "    keys = [key for key in d.keys() if key not in keys_to_ignore]\n",
    "    lengths = [len(d[k]) for k in keys]\n",
    "    assert len(set(lengths)) == 1\n",
    "    length = lengths[0]\n",
    "    res = [{k: d[k][i] for k in keys} for i in range(length)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad931ef-cf57-47c7-9cfd-0ab3db015747",
   "metadata": {},
   "source": [
    "## Running and evaluationg the pre-trained relation model\n",
    "\n",
    "Now that the setup is out of the way. We can actually run the model and evaluate it with a pre-trained BERT-based model on the SciERC dataset.\n",
    "\n",
    "We will do that in the run relation notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
